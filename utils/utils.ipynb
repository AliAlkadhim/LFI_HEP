{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b14509-5d4a-49df-974c-ea673612b374",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_params/best_params_Test_Trials.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_528323/1018507043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m \u001b[0mBEST_PARAMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_params/best_params_Test_Trials.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEST_PARAMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_params/best_params_Test_Trials.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np; import pandas as pd\n",
    "import scipy as sp; import scipy.stats as st\n",
    "import torch; import torch.nn as nn\n",
    "#use numba's just-in-time compiler to speed things up\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler; from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp; import matplotlib.pyplot as plt; \n",
    "#reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use('seaborn-deep')\n",
    "mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15; font_axes=15\n",
    "# %matplotlib inline\n",
    "import copy; import sys; import os\n",
    "from IPython.display import Image, display\n",
    "import optuna\n",
    "#sometimes jupyter doesnt initialize MathJax automatically for latex, so do this\n",
    "import ipywidgets as wid; wid.HTMLMath('$\\LaTeX$')\n",
    "\n",
    "\n",
    "\n",
    "def import_base_stack():\n",
    "    import numpy as np; import pandas as pd\n",
    "    import scipy as sp; from numba import njit\n",
    "\n",
    "def DR(s, theta):\n",
    "    return sp.special.gammainc(s, theta)\n",
    "\n",
    "\n",
    "def DL(s, theta):\n",
    "    return 1 - sp.special.gammainc(s+1, theta)\n",
    "\n",
    "\n",
    "def L_prof(n,m,theta):\n",
    "    k=1\n",
    "    k1 = k+1\n",
    "    k2 = 0.5/k1\n",
    "    g = n+m - k1*theta\n",
    "    nu_hat = k2* (g+ np.sqrt(g*g +4*k1*m*theta))\n",
    "    p1 = st.poisson.pmf(n, mu = theta + nu_hat)\n",
    "    p2 = st.poisson.pmf(m, mu = k * nu_hat)\n",
    "    \n",
    "    return p1*p2\n",
    "\n",
    "\n",
    "def theta_hat(n,m, MLE=True):\n",
    "    theta_hat = n-m\n",
    "    \n",
    "    if not MLE:\n",
    "        theta_hat = theta_hat * (theta_hat > 0)\n",
    "    return theta_hat\n",
    "\n",
    "# @njit\n",
    "def lambda_test(theta, n, m, MLE=True):\n",
    "    Ln = L_prof(n,m,theta)\n",
    "    Ld = L_prof(n,m, theta_hat(n,m, MLE))\n",
    "    lambda_  = -2*np.log(Ln/Ld)\n",
    "    return np.array(lambda_)\n",
    "\n",
    "\n",
    "chi2_exp_size=40000\n",
    "\n",
    "def run_sim(theta, nu, MLE, lambda_size):\n",
    "    \"\"\"Sample n ~ Pois(theta+nu), \n",
    "              m ~ Pois(nu), \n",
    "    and compute \n",
    "              lambda(theta, n, m)\n",
    "              \n",
    "    return: (n, m, lambda_), where each are np arrays of length lambda_size\n",
    "    \"\"\"\n",
    "    n = st.poisson.rvs(theta+nu, size=lambda_size)\n",
    "    m = st.poisson.rvs(nu, size=lambda_size)\n",
    "    lambda_ = lambda_test(theta, n, m, MLE=MLE)\n",
    "    return (n, m, lambda_)\n",
    "\n",
    "def run_sims(points, MLE):\n",
    "    \"\"\"\n",
    "    Run an entire simulation, that is, generate n and m from \n",
    "    run_sim above, and calculate lambda, for\n",
    "    \n",
    "    input: a tuple of (theta, nu) scalars\n",
    "    \n",
    "    Reurns:df, lambda_results\n",
    "    \n",
    "    where lambda_results is a list of tuples \n",
    "        (n, m, lambda_, theta, nu)\n",
    "    and df is just a dataframe of [n,m,lambda,theta,nu]\n",
    "\n",
    "    \"\"\"\n",
    "    lambda_results=[]\n",
    "    df=pd.DataFrame()\n",
    "    for p in points:\n",
    "        theta, nu = p\n",
    "        df['theta']=theta\n",
    "        df['nu']=nu\n",
    "        n, m, lambda_ = run_sim(theta, nu, MLE, lambda_size =chi2_exp_size)\n",
    "        df['n'] = n\n",
    "        df['m'] = m\n",
    "        df['lambda']=lambda_\n",
    "        lambda_results.append((n, m, lambda_, theta, nu))\n",
    "    \n",
    "        print( '\\n \\n (theta, nu) =  (%.f, %.f) \\n ' % (theta, nu) )\n",
    "        print(f'\\t \\t with associated n =  {n}, \\n \\n \\t \\t m = {m}, \\n \\n \\t \\t lambda = {lambda_}'  )\n",
    "    return df, lambda_results\n",
    "\n",
    "def plot_one(lambda_, theta, nu, ax):\n",
    "    \"\"\"Histogram the CDF of  lambda_t = -2log(Lp(theta)/Lp(theta_hat)), \n",
    "    for a given (fixed) theta and nu.\n",
    "    Also, plot the actual CDF of a chi^2 distribution with 1 free parameter \n",
    "    (since only theta is left after we profile nu) \"\"\"\n",
    "    ftsize = 16; xmin= 0; xmax= 10\n",
    "    ymin= 0; ymax= 1\n",
    "    x_range = (xmin, xmax)\n",
    "    y_range = (ymin, ymax)\n",
    "    ax.set_xlim(x_range); ax.set_ylim(y_range)\n",
    "    ax.set_xlabel(r'$\\lambda \\left(\\theta,\\hat{\\nu}(\\theta) \\mid n, m \\right)$',fontsize=ftsize)\n",
    "    ax.set_ylabel(r'cdf$(\\lambda)$', fontsize=ftsize)\n",
    "    ##########HISTOGRAM CDF OF LAMBDA####################\n",
    "    ax.hist(lambda_, bins=5*xmax, range=x_range,\n",
    "    color=(0.8,0.8,0.9),\n",
    "    density=True, cumulative=True,\n",
    "    histtype='stepfilled', edgecolor='black', label=r'CDF$(\\lambda)$')\n",
    "    ############################################################\n",
    "    ########### HISTOGRAM CDF OF THE CHI2 OF OF X WITH 1 DOF\n",
    "    #x is not theta, that's the whole point of Wilks thm, x is an arbitrary RV\n",
    "    x = np.arange(0, xmax, 0.2)\n",
    "    y = st.chi2.cdf(x, 1)\n",
    "    ax.plot(x, y, color='blue',\n",
    "    linewidth=2, label=r'CDF$(\\chi^2_1)$')\n",
    "    # annotate\n",
    "    xwid = (xmax-xmin)/12\n",
    "    ywid = (ymax-ymin)/12\n",
    "    xpos = xmin + xwid/2\n",
    "    ypos = ymin + ywid*2\n",
    "    ax.text(xpos, ypos,\n",
    "    r'$ \\theta = %d, \\nu = %d$' % (theta, nu),\n",
    "    fontsize=ftsize)\n",
    "    ax.legend(loc='upper right',fontsize=15)\n",
    "    \n",
    "\n",
    "def observe_test_statistic_pivotality(points, lambda_size, savefig=False):\n",
    "    \"\"\"Histogram the CDF of  lambda_t = -2log(Lp(theta)/Lp(theta_hat)), \n",
    "    for a given (fixed) theta and nu.\n",
    "    Also, plot the actual CDF of a chi^2 distribution with 1 free parameter \n",
    "    (since only theta is left after we profile nu) \"\"\"\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,5), sharey=True)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    title_size=15\n",
    "    for point in points:\n",
    "        _, _, lambda_MLE = run_sim(theta=point[0], nu=point[1], MLE=True, lambda_size=chi2_exp_size)\n",
    "\n",
    "\n",
    "        ftsize = 16; xmin= 0; xmax= 10\n",
    "        ymin= 0; ymax= 1\n",
    "        x_range = (xmin, xmax)\n",
    "        y_range = (ymin, ymax)\n",
    "        \n",
    "        ax[0].hist(lambda_MLE, bins=5*xmax, range=x_range,\n",
    "        # color=(0.8,0.8,0.9),\n",
    "                   alpha=0.3,\n",
    "        density=True, cumulative=True,\n",
    "        histtype='stepfilled', edgecolor='black', \n",
    "        label=r'CDF$\\left(\\lambda_{NP}(\\theta=%s,\\nu=%s ) \\right)$' % (str(point[0]), str(point[1])) )\n",
    "        \n",
    "        ############################################################\n",
    "        ########### HISTOGRAM CDF OF THE CHI2 OF OF X WITH 1 DOF\n",
    "        #x is not theta, that's the whole point of Wilks thm, x is an arbitrary RV\n",
    "        x = np.arange(0, xmax, 0.2)\n",
    "        y = st.chi2.cdf(x, 1)\n",
    "        ax[0].plot(x, y, \n",
    "                   # color='blue',\n",
    "                    linewidth=1, \n",
    "                   label=r'CDF$ \\left(\\chi^2_1(\\theta=%s) \\right)$' % str(point[0]))\n",
    "        \n",
    "        ax[0].set_title('MLE=True',fontsize=title_size)\n",
    "        #####################Do the same for non-MLE\n",
    "        \n",
    "            \n",
    "    for point in points:\n",
    "        _, _, lambda_nonMLE = run_sim(theta=point[0], nu=point[1], MLE=False, lambda_size=chi2_exp_size)\n",
    "\n",
    "\n",
    "        ftsize = 16; xmin= 0; xmax= 10\n",
    "        ymin= 0; ymax= 1\n",
    "        x_range = (xmin, xmax)\n",
    "        y_range = (ymin, ymax)\n",
    "        \n",
    "        ax[1].hist(lambda_nonMLE, bins=5*xmax, range=x_range,\n",
    "        # color=(0.8,0.8,0.9),\n",
    "                   alpha=0.3,\n",
    "        density=True, cumulative=True,\n",
    "        histtype='stepfilled', edgecolor='black', \n",
    "        label=r'CDF$\\left(\\lambda_{NP}(\\theta=%s,\\nu=%s ) \\right)$' % (str(point[0]), str(point[1])) )\n",
    "        \n",
    "        ############################################################\n",
    "        ########### HISTOGRAM CDF OF THE CHI2 OF OF X WITH 1 DOF\n",
    "        #x is not theta, that's the whole point of Wilks thm, x is an arbitrary RV\n",
    "        x = np.arange(0, xmax, 0.2)\n",
    "        y = st.chi2.cdf(x, 1)\n",
    "        ax[1].plot(x, y, \n",
    "                   # color='blue',\n",
    "                    linewidth=1, \n",
    "                   label=r'CDF$ \\left(\\chi^2_1(\\theta=%s) \\right)$' % str(point[0]))\n",
    "        \n",
    "        \n",
    "        ax[1].set_title('MLE=False',fontsize=title_size)\n",
    "        \n",
    "        # annotate\n",
    "        xwid = (xmax-xmin)/12\n",
    "        ywid = (ymax-ymin)/12\n",
    "        xpos = xmin + xwid/2\n",
    "        ypos = ymin + ywid*2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    for i in range(2):\n",
    "        ax[i].set_xlabel(r'$\\lambda_{NP} \\left(\\theta,\\hat{\\nu}(\\theta) \\mid n, m \\right)$',fontsize=ftsize)\n",
    "        ax[i].set_ylabel(r'cdf$(\\lambda_{NP})$', fontsize=ftsize)\n",
    "        ax[i].legend(loc='lower right',fontsize=9)\n",
    "        ax[i].set_xlim(x_range)\n",
    "        ax[i].set_ylim(y_range)\n",
    "        \n",
    "        \n",
    "    if savefig:\n",
    "        plt.savefig('images/lambda_NP_observe_pivotality.png')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def generate_training_data(Bprime, MLE, save_data=False):\n",
    "    \"\"\"Generate the training data, that is, features=[theta, nu, N, M], targets=Z\"\"\"\n",
    "    #sample theta and nu from uniform(0,20)\n",
    "    theta = st.uniform.rvs(thetaMin, thetaMax, size=Bprime)\n",
    "    # nu = st.uniform.rvs(nuMin, nuMax, size=Bprime)\n",
    "    nu= st.uniform.rvs(numin, numax, size=Bprime)\n",
    "    #n,m ~ F_{\\theta,\\nu}, ie our simulator. sample n from a Poisson with mean theta+nu \n",
    "    n = st.poisson.rvs(theta+ nu, size=Bprime)\n",
    "    #sample m from a poisson with mean nu\n",
    "    m = st.poisson.rvs(nu, size=Bprime)\n",
    "    #sample our observed counts (N,M), which take the place of D\n",
    "    N = np.random.randint(Nmin, Nmax, size=Bprime)\n",
    "    M = np.random.randint(Mmin, Mmax, size=Bprime)\n",
    "    theta_hat_ = theta_hat(N,M, MLE)\n",
    "    SUBSAMPLE=10\n",
    "    print('n=', n[:SUBSAMPLE])\n",
    "    print('m=', m[:SUBSAMPLE])\n",
    "    print('N=', N[:SUBSAMPLE])\n",
    "    print('M=', M[:SUBSAMPLE])\n",
    "    lambda_gen = lambda_test(theta, n, m, MLE)\n",
    "    print('lambda_gen= ', lambda_gen[:SUBSAMPLE])\n",
    "    lambda_D = lambda_test(theta, N, M, MLE)\n",
    "    print('lambda_D= ', lambda_D[:SUBSAMPLE])\n",
    "    #if lambda_gen <= lambda_D: Z=1, else Z=0\n",
    "    Z = (lambda_gen <= lambda_D).astype(np.int32)\n",
    "    \n",
    "    data_2_param = {'Z' : Z, 'theta' : theta, 'nu': nu, 'theta_hat': theta_hat_, 'N':N, 'M':M}\n",
    "\n",
    "    data_2_param = pd.DataFrame.from_dict(data_2_param)\n",
    "    if save_data:\n",
    "        data_2_param.to_csv('data/two_parameters_theta_%s_%s_%sk_Examples_MLE_%s.csv' %\\\n",
    "                            (str(thetaMin), str(thetaMax), str(int(Bprime/1000)), str(MLE)) )\n",
    "\n",
    "    print('\\n')\n",
    "    print(data_2_param.describe())\n",
    "    return data_2_param\n",
    "\n",
    "\n",
    "def make_hist_data(Bprime,\n",
    "              thetamin, thetamax,\n",
    "              nu, N, M,\n",
    "                nbins,\n",
    "             MLE=True):\n",
    "\n",
    "    theta = st.uniform.rvs(thetamin, thetamax, size=Bprime)\n",
    "    n = st.poisson.rvs(theta + nu, size=Bprime)\n",
    "    m = st.poisson.rvs(nu, size=Bprime)\n",
    "    \n",
    "    Z = (lambda_test(theta, n, m, MLE=MLE) < \n",
    "         lambda_test(theta, N, M, MLE=MLE)).astype(np.int32)\n",
    "\n",
    "    thetarange = (thetamin, thetamax)\n",
    "    # bins = binsize(Bprime)\n",
    "\n",
    "    # weighted histogram   (count the number of ones per bin)\n",
    "    y1, bb = np.histogram(theta, \n",
    "                          bins=nbins, \n",
    "                          range=thetarange, \n",
    "                          weights=Z)\n",
    "    \n",
    "    # unweighted histogram (count number of ones and zeros per bin)\n",
    "    yt, _ = np.histogram(theta, \n",
    "                         bins=nbins, \n",
    "                         range=thetarange)\n",
    "\n",
    "    y =  y1 / yt    \n",
    "    \n",
    "    return y, bb\n",
    "\n",
    "\n",
    "def plot_one_hist(Bprime, thetamin, thetamax, nu, N, M, MLE, nbins, ax):\n",
    "    counts, bins= make_hist_data(Bprime,\n",
    "              thetamin, thetamax,\n",
    "              nu, N, M,\n",
    "            nbins,\n",
    "             MLE)\n",
    "    bin_centers = (bins[1:]+bins[:-1])/2\n",
    "    ax.plot(bin_centers, counts, label= r'$\\mathbf{h}$ Example', lw=3)\n",
    "    ax.set_xlabel(r'$\\theta$',fontsize=font_axes)\n",
    "    ax.set_ylabel(r'$E(Z|\\theta,\\nu)$',fontsize=font_axes)\n",
    "    ax.legend(loc='center right',fontsize=font_legend)\n",
    "    \n",
    "    \n",
    "def plot_data_one_nu(Bprime, thetamin, thetamax, nu, D, MLE, \n",
    "              FONTSIZE=15,\n",
    "              func=None,\n",
    "              fgsize=(10, 6)):\n",
    "    \n",
    "    # make room for 6 sub-plots\n",
    "    fig, ax = plt.subplots(nrows=2, \n",
    "                           ncols=3, \n",
    "                           figsize=fgsize)\n",
    "    \n",
    "    # padding\n",
    "    plt.subplots_adjust(hspace=0.01)\n",
    "    plt.subplots_adjust(wspace=0.20)\n",
    "    \n",
    "    # use flatten() to convert a numpy array of \n",
    "    # shape (nrows, ncols) to a 1-d array. \n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for j, (N, M) in enumerate(D):\n",
    "        \n",
    "        y, bb = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=200,\n",
    "                              MLE=True)\n",
    "    \n",
    "        ax[j].set_xlim(thetamin, thetamax-5)\n",
    "        ax[j].set_ylim(0, 1)\n",
    "        ax[j].set_xlabel(r'$\\theta$', fontsize=FONTSIZE)\n",
    "        ax[j].set_ylabel(r'$E(Z|\\theta, \\nu)$', fontsize=FONTSIZE)\n",
    "        \n",
    "        x = (bb[1:]+bb[:-1])/2\n",
    "        ax[j].plot(x, y, 'b', lw=2, label='$\\mathbf{h}$, MLE', alpha=0.3)\n",
    "        #h is histogram approximation\n",
    "\n",
    "        y_nonMLE, bb_nonMLE = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=200,\n",
    "                              MLE=False)\n",
    "        \n",
    "        \n",
    "        x_nonMLE = (bb_nonMLE[1:]+bb_nonMLE[:-1])/2\n",
    "        ax[j].plot(x_nonMLE, y_nonMLE, 'r', lw=2, label='$\\mathbf{h}$, non-MLE',alpha=0.3)\n",
    "        \n",
    "        \n",
    "        if func:\n",
    "            p, _ = func(nu, N, M)\n",
    "            ax[j].plot(x, p, 'r', lw=2, label='f')\n",
    "            #f is model approximation\n",
    "        \n",
    "        ax[j].grid(True, which=\"both\", linestyle='-')\n",
    "        ax[j].text(5.1, 0.42, r'$N, M = %d, %d$' % (N, M), fontsize=font_legend-3\n",
    "                   # fontsize=FONTSIZE\n",
    "                  ) \n",
    "\n",
    "        ax[j].text(5.1, 0.30, r'$\\nu = %5.1f$' % nu, fontsize=font_legend-3\n",
    "                   # fontsize=FONTSIZE\n",
    "                  ) \n",
    "\n",
    "        ax[j].legend(loc='upper right',fontsize=font_legend-3)\n",
    "        \n",
    "    # hide unused sub-plots\n",
    "    for k in range(j+1, len(ax)):\n",
    "        ax[k].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def getwholedata(MLE_or_nonMLE, valid=False):\n",
    "    if MLE:\n",
    "        data = pd.read_csv('data/two_parameters_theta_0_20_1000k_Examples_MLE_True.csv', \n",
    "                     # nrows=SUBSAMPLE,\n",
    "                     usecols=['theta', 'nu', 'theta_hat', 'N', 'M']\n",
    "                    )\n",
    "        \n",
    "    else:\n",
    "        data = pd.read_csv('data/two_parameters_theta_0_20_1000k_Examples_MLE_False.csv', \n",
    "             # nrows=SUBSAMPLE,\n",
    "             usecols=['theta', 'nu', 'theta_hat', 'N', 'M']\n",
    "            )\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "    #split the train data (0.8 of whole set) again into 0.8*0.8=0.64 of whole set\n",
    "    \n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data  = test_data.reset_index(drop=True)\n",
    "\n",
    "    target='Z'\n",
    "    source = ['theta','nu','theta_hat','N','M']\n",
    "\n",
    "    train_t, train_x = split_t_x(train_data, target=target, source=source)\n",
    "    test_t,  test_x  = split_t_x(test_data,  target=target, source=source)\n",
    "    print('train_t shape = ', train_t.shape, '\\n')\n",
    "    print('train_x shape = ', train_x.shape, '\\n')\n",
    "    \n",
    "    if valid:\n",
    "        #if you want to also make a validation data set\n",
    "        train_data, valid_data = train_test_split(train_data, test_size=0.2)\n",
    "        valid_data = valid_data.reset_index(drop=True)\n",
    "        valid_t, valid_x = split_t_x(valid_data, target=target, source=source)\n",
    "\n",
    "        \n",
    "    return train_t, train_x, test_t,  test_x\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs=4, n_nodes=20, n_layers=5):\n",
    "\n",
    "        # call constructor of base (or super, or parent) class\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "        \n",
    "        # create input layer\n",
    "        self.layer0 = nn.Linear(n_inputs, n_nodes)\n",
    "        self.layers.append(self.layer0)\n",
    "\n",
    "        # create \"hidden\" layers\n",
    "        for l in range(1, n_layers):\n",
    "            cmd = 'self.layer%d = nn.Linear(%d, %d)' % \\\n",
    "            (l, n_nodes, n_nodes)\n",
    "            exec(cmd)\n",
    "            cmd = 'self.layers.append(self.layer%d)' % l\n",
    "            exec(cmd)\n",
    "          \n",
    "        # create output layer\n",
    "        cmd = 'self.layer%d = nn.Linear(%d, 1)' % (n_layers, n_nodes)\n",
    "        exec(cmd)\n",
    "        cmd = 'self.layers.append(self.layer%d)' % n_layers\n",
    "        exec(cmd)\n",
    "\n",
    "    # define (required) method to compute output of network\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            y = layer(y)\n",
    "            y = torch.relu(y)\n",
    "        y = self.layers[-1](y)\n",
    "        y = torch.sigmoid(y)\n",
    "        return y\n",
    "\n",
    "def average_quadratic_loss(f, t, x):\n",
    "    # f and t must be of the same shape\n",
    "    return  torch.mean((f - t)**2)\n",
    "def average_loss(f, t):\n",
    "    # f and t must be of the same shape\n",
    "    return  torch.mean((f - t)**2)\n",
    "\n",
    "def validate(model, avloss, inputs, targets):\n",
    "    # make sure we set evaluation mode so that any training specific\n",
    "    # operations are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and t\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).float()\n",
    "        # remember to reshape!\n",
    "        o = model(x).reshape(t.shape)\n",
    "    return avloss(o, t, x)\n",
    "\n",
    "\n",
    "def get_features_training_batch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"batch_size\" integers from \n",
    "    # the range [0, length-1] corresponding to the\n",
    "    # row indices.\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    # batch_x.T[-1] = np.random.uniform(0, 1, batch_size)\n",
    "    return (batch_x, batch_t)\n",
    "\n",
    "\n",
    "def train(model, optimizer, avloss,\n",
    "          batch_size, \n",
    "          n_iterations, traces, \n",
    "          step, window, MLE):\n",
    "    \n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    \n",
    "\n",
    "    \n",
    "    if MLE:\n",
    "        train_t, train_x, test_t,  test_x = getwholedata(MLE_or_nonMLE=True, valid=False)\n",
    "    else:\n",
    "        train_t, train_x, test_t,  test_x = getwholedata(MLE_or_nonMLE=False, valid=False)\n",
    "        \n",
    "    n = len(test_x)\n",
    "    print('Iteration vs average loss')\n",
    "    print(\"%10s\\t%10s\\t%10s\" % \\\n",
    "          ('iteration', 'train-set', 'valid-set'))\n",
    "    \n",
    "    # training_set_features, training_set_targets, evaluation_set_features, evaluation_set_targets = get_data_sets(simulate_data=False, batchsize=batch_size)\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "\n",
    "        # set mode to training so that training specific \n",
    "        # operations such as dropout are enabled.\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        \n",
    "        #Harrison-like Loader\n",
    "        batch_x, batch_t = get_features_training_batch(train_x, train_t, batch_size)\n",
    "        \n",
    "        #Or Ali's Loader\n",
    "        # batch_x, batch_t = next(training_set_features()), next(training_set_targets())\n",
    "        # batch_x_eval, batch_t_eval = next(evaluation_set_features()), next(evaluation_set_targets())\n",
    "\n",
    "        with torch.no_grad(): # no need to compute gradients \n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).float()      \n",
    "\n",
    "\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "   \n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        empirical_risk.backward()   # compute gradients\n",
    "        \n",
    "        # finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer.step()            # move one step\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            \n",
    "            \n",
    "            #using Harrison-like loader\n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n]) \n",
    "            acc_v = validate(model, avloss, test_x[:n], test_t[:n])\n",
    "            \n",
    "            #using Ali's loader\n",
    "            # acc_t = validate(model, avloss, batch_x, batch_t) \n",
    "            # acc_v = validate(model, avloss, batch_x_eval, batch_t_eval)\n",
    "            \n",
    "\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "            # compute running average for validation data\n",
    "            len_yy_v = len(yy_v)\n",
    "            if   len_yy_v < window:\n",
    "                yy_v_avg.append( yy_v[-1] )\n",
    "            elif len_yy_v == window:\n",
    "                yy_v_avg.append( sum(yy_v) / window )\n",
    "            else:\n",
    "                acc_v_avg  = yy_v_avg[-1] * window\n",
    "                acc_v_avg += yy_v[-1] - yy_v[-window-1]\n",
    "                yy_v_avg.append(acc_v_avg / window)\n",
    "                        \n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % \\\n",
    "                      (xx[-1], yy_t[-1], yy_v[-1]))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "                    \n",
    "                print(\"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\" % \\\n",
    "                          (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]), \n",
    "                      end='')\n",
    "            \n",
    "    print()      \n",
    "    return (xx, yy_t, yy_v, yy_v_avg)\n",
    "\n",
    "def plot_average_loss(traces, ftsize=18,save_loss_plots=False):\n",
    "    \n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(6, 4.5))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    ax.plot(xx, yy_t, 'b', lw=2, label='Training')\n",
    "    ax.plot(xx, yy_v, 'r', lw=2, label='Validation')\n",
    "    #ax.plot(xx, yy_v_avg, 'g', lw=2, label='Running average')\n",
    "\n",
    "    ax.set_xlabel('Iterations', fontsize=ftsize)\n",
    "    ax.set_ylabel('average loss', fontsize=ftsize)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, which=\"both\", linestyle='-')\n",
    "    ax.legend(loc='upper right')\n",
    "    if save_loss_plots:\n",
    "        plt.savefig('images/loss_curves/IQN_'+N+T+'_Consecutive_2.png')\n",
    "        print('\\nloss curve saved in images/loss_curves/IQN_'+N+target+'_Consecutive.png')\n",
    "    # if show_loss_plots:\n",
    "    plt.show()\n",
    "\n",
    "class RegularizedRegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "\n",
    "        # ONLY IF ITS A CLASSIFICATION, ADD SIGMOID\n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class Engine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer, batch_size):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    # @staticmethod\n",
    "    # def loss_fun(targets, outputs):\n",
    "    #   tau = torch.rand(outputs.shape)\n",
    "    #   return torch.mean(torch.where(targets >= outputs, \n",
    "    #                                   tau * (targets - outputs), \n",
    "    #                                   (1 - tau)*(outputs - targets)))\n",
    "\n",
    "#     This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, \n",
    "#     by combining the operations into one layer\n",
    "\n",
    "    def train(self, x, t):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for iteration in range(n_iterations):\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_x, batch_t = get_features_training_batch(x, t,  self.batch_size)#x and t are train_x and train_t\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            inputs=torch.from_numpy(batch_x).float()\n",
    "            targets=torch.from_numpy(batch_t).float()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = average_quadratic_loss(outputs, targets, inputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "\n",
    "        return final_loss / self.batch_size\n",
    "    \n",
    "    def evaluate(self, x, t):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for iteration in range(n_iterations):\n",
    "            batch_x, batch_t = get_features_training_batch(x, t,  self.batch_size)#x and t are train_x and train_t\n",
    "\n",
    "            # with torch.no_grad():            \n",
    "            inputs=torch.from_numpy(batch_x).float()\n",
    "            targets=torch.from_numpy(batch_t).float()\n",
    "            outputs = self.model(inputs)\n",
    "            loss =average_quadratic_loss(outputs, targets, inputs)\n",
    "            final_loss += loss.item()\n",
    "        return final_loss / self.batch_size\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS=1\n",
    "def run_train(params, save_model=False):\n",
    "    \"\"\"For tuning the parameters\"\"\"\n",
    "\n",
    "    model =  RegularizedRegressionModel(\n",
    "              nfeatures=sample_x.shape[1], \n",
    "                ntargets=1,\n",
    "                nlayers=params[\"nlayers\"], \n",
    "                hidden_size=params[\"hidden_size\"],\n",
    "                dropout=params[\"dropout\"]\n",
    "                )\n",
    "    # print(model)\n",
    "    \n",
    "\n",
    "    learning_rate= params[\"learning_rate\"]\n",
    "    optimizer_name = params[\"optimizer_name\"]\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"]) \n",
    "    \n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    eng=Engine(model, optimizer, batch_size=params[\"batch_size\"])\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter=10\n",
    "    early_stopping_coutner=0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_x, train_t)\n",
    "        valid_loss=eng.evaluate(test_x, test_t)\n",
    "\n",
    "        print(f\"{epoch} \\t {train_loss} \\t {valid_loss}\")\n",
    "        if valid_loss<best_loss:\n",
    "            best_loss=valid_loss\n",
    "            if save_model:\n",
    "                model.save(model.state_dict(), \"model_m.bin\")\n",
    "        else:\n",
    "            early_stopping_coutner+=1\n",
    "        if early_stopping_coutner > early_stopping_iter:\n",
    "            break\n",
    "    return best_loss\n",
    "\n",
    "# run_train()\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "      \"nlayers\": trial.suggest_int(\"nlayers\",1,13),      \n",
    "      \"hidden_size\": trial.suggest_int(\"hidden_size\", 2, 130),\n",
    "      \"dropout\": trial.suggest_float(\"dropout\", 0.1,0.5),\n",
    "      \"optimizer_name\" : trial.suggest_categorical(\"optimizer_name\", [\"Adam\", \"RMSprop\"]),\n",
    "      \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-2),\n",
    "      \"batch_size\": trial.suggest_int(\"batch_size\", 1000, 10000)\n",
    "\n",
    "    }\n",
    "    # all_losses=[]\n",
    "\n",
    "    temp_loss = run_train(params,save_model=False)\n",
    "    # all_losses.append(temp_loss)\n",
    "    return temp_loss\n",
    "\n",
    "def tune_hyperparameters():\n",
    "    print('Getting best hyperparameters')\n",
    "    study=optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    best_trial = study.best_trial\n",
    "    print('best model parameters', best_trial.params)\n",
    "\n",
    "    best_params=best_trial.params#this is a dictionary\n",
    "    filename='best_params/best_params_Test_Trials.csv'\n",
    "    param_df=pd.DataFrame({\n",
    "                            'n_layers':best_params[\"nlayers\"], \n",
    "                            'hidden_size':best_params[\"hidden_size\"], \n",
    "                            'dropout':best_params[\"dropout\"],\n",
    "                            'optimizer_name':best_params[\"optimizer_name\"],\n",
    "                            'learning_rate': best_params[\"learning_rate\"], \n",
    "                            'batch_size':best_params[\"batch_size\"] },\n",
    "                                    index=[0]\n",
    "    )\n",
    "\n",
    "    param_df.to_csv(filename)   \n",
    "    \n",
    "    \n",
    "BEST_PARAMS = pd.read_csv('best_params/best_params_Test_Trials.csv')\n",
    "print(BEST_PARAMS)\n",
    "\n",
    "n_layers = int(BEST_PARAMS[\"n_layers\"]) \n",
    "hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "\n",
    "def initiate_whose_model(Ali_or_Harrison, MLE):\n",
    "    whose_model='Ali'\n",
    "\n",
    "    if whose_model=='Harrison':\n",
    "        n_layers=5\n",
    "        hidden_size=5\n",
    "        dropout=0\n",
    "        learning_rate=int(1e-3)\n",
    "        batch_size=64\n",
    "        optimizer     = torch.optim.Adam(model.parameters(), lr=int(1e-3)) \n",
    "        model=Model()\n",
    "    elif whose_model=='Ali':\n",
    "        n_layers = int(BEST_PARAMS[\"n_layers\"]) \n",
    "        hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "        dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "        optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "        learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "        batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "        model =  RegularizedRegressionModel(\n",
    "            nfeatures=sample_x.shape[1], \n",
    "            ntargets=1,\n",
    "            nlayers=n_layers, \n",
    "            hidden_size=hidden_size, \n",
    "            dropout=dropout\n",
    "            )\n",
    "        optimizer = getattr(torch.optim, str(optimizer_name) )(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    return n_layers, hidden_size, dropout, optimizer_name, learning_rate, batch_size, model, optimizer\n",
    "\n",
    "\n",
    "def make_eval_data(Bprime, train_df, nu, N, M, nbins):\n",
    "    #if MLE true, load the model that was trained on MLE data and vice versa\n",
    "    # N, M = D\n",
    "    # nbins=NBINS\n",
    "    # thetamin,thetamax=0,20\n",
    "    thetamin=train_df['theta'].min()\n",
    "    thetamax=train_df['theta'].max()\n",
    "    thetastep = (thetamax-thetamin) / nbins\n",
    "    bb    = np.arange(thetamin, thetamax+thetastep, thetastep)#this is just making a vector of thetas\n",
    "    X     = (bb[1:] + bb[:-1])/2\n",
    "    tensor = torch.Tensor([[x, nu, theta_hat(N, M, MLE=True), N, M] for x in X])\n",
    "    return tensor, X.ravel()\n",
    "\n",
    "def usemodel(Bprime, train_df, nu, N,M, MLE, nbins):\n",
    "    \n",
    "    #Generate evaluation data at those fixed nu, N, M values\n",
    "    eval_data, eval_bins =make_eval_data(Bprime,train_df,nu, N,M, nbins)#eval data is indipendent of MLE, since its just constants witha theta variable\n",
    "\n",
    "    # if MLE==True:\n",
    "    #     model=model\n",
    "    #else load the model trained on non-MLE data\n",
    "    # PATH='models/MLE_TRUE_Regressor_200.0K_training_iter.pt'\n",
    "    \n",
    "    #LOAD TRAINED MODEL\n",
    "    with_theta_hat=False\n",
    "    if MLE:\n",
    "        \n",
    "        PATH= 'models/MLE_TRUE_Regressor_200.0K_training_iter.pt'\n",
    "        PATH= 'models/MLE_True_Regressor_100.0K_training_iter_with_theta_hat.pt'\n",
    "        \n",
    "    else:\n",
    "        PATH= 'models/MLE_False_Regressor_200.0K_training_iter.pt'\n",
    "        PATH= 'models/MLE_False_Regressor_100.0K_training_iter_with_theta_hat.pt'\n",
    "    n_layers = int(BEST_PARAMS[\"n_layers\"]) \n",
    "    hidden_size = int(BEST_PARAMS[\"hidden_size\"])\n",
    "    dropout = float(BEST_PARAMS[\"dropout\"])\n",
    "    optimizer_name = BEST_PARAMS[\"optimizer_name\"].to_string().split()[1]\n",
    "    learning_rate =  float(BEST_PARAMS[\"learning_rate\"])\n",
    "    batch_size = int(BEST_PARAMS[\"batch_size\"])\n",
    "    model =  RegularizedRegressionModel(\n",
    "        nfeatures=sample_x.shape[1], \n",
    "        ntargets=1,\n",
    "        nlayers=n_layers, \n",
    "        hidden_size=hidden_size, \n",
    "        dropout=dropout\n",
    "        )\n",
    "    #EVALUATE AT AT EVAL_DATA\n",
    "    model.load_state_dict(torch.load(PATH) )\n",
    "    model.eval()\n",
    "    return model(eval_data).detach().numpy(), eval_bins\n",
    "\n",
    "# sample_x=train_df_MLE\n",
    "def plot_data_one_nu_with_model(Bprime, thetamin, thetamax, nu, D, MLE, \n",
    "                     NBINS,\n",
    "              FONTSIZE=15,\n",
    "              func=None,\n",
    "              fgsize=(10, 6), save_image=False):\n",
    "    \n",
    "    # make room for 6 sub-plots\n",
    "    fig, ax = plt.subplots(nrows=2, \n",
    "                           ncols=3, \n",
    "                           figsize=fgsize)\n",
    "    \n",
    "    # padding\n",
    "    plt.subplots_adjust(hspace=0.01)\n",
    "    plt.subplots_adjust(wspace=0.20)\n",
    "    \n",
    "    # use flatten() to convert a numpy array of \n",
    "    # shape (nrows, ncols) to a 1-d array. \n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for j, (N, M) in enumerate(D):\n",
    "        \n",
    "        y, bb = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=NBINS,\n",
    "                              MLE=True)\n",
    "    \n",
    "        ax[j].set_xlim(thetamin-0.5, thetamax-5)\n",
    "        ax[j].set_ylim(0, 1.03)\n",
    "        ax[j].set_xlabel(r'$\\mathbf{\\theta}$', fontsize=FONTSIZE-3)\n",
    "        ax[j].set_ylabel(r'$\\mathbf{E(Z|\\theta, \\nu)}$', fontsize=FONTSIZE-3)\n",
    "        \n",
    "        x = (bb[1:]+bb[:-1])/2\n",
    "        ax[j].plot(x, y, 'b', lw=2, label='$\\mathbf{h}$ MLE', alpha=0.4)\n",
    "        #h is histogram approximation\n",
    "\n",
    "        y_nonMLE, bb_nonMLE = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=NBINS,\n",
    "                              MLE=False)\n",
    "        \n",
    "        \n",
    "        x_nonMLE = (bb_nonMLE[1:]+bb_nonMLE[:-1])/2\n",
    "        ax[j].plot(x_nonMLE, y_nonMLE, 'r', lw=2, label='$\\mathbf{h}$ non-MLE',alpha=0.4)\n",
    "        \n",
    "        \n",
    "        if func:\n",
    "            train_df_MLE = load_train_df(MLE=True)\n",
    "            train_df_nonMLE = load_train_df(MLE=False)\n",
    "            \n",
    "            f_MLE, f_bins_MLE = func(Bprime, train_df_MLE, nu, N, M, MLE=True, nbins=NBINS)\n",
    "            ax[j].plot(f_bins_MLE, f_MLE, color='g', lw=2, label='$\\mathbf{f}$ MLE', alpha=0.4)\n",
    "            #f is model approximation\n",
    "            \n",
    "            f_nonMLE, f_bins_nonMLE = func(Bprime, train_df_nonMLE, nu, N, M, MLE=False, nbins=NBINS)\n",
    "            ax[j].plot(f_bins_nonMLE, f_nonMLE, color='c', lw=2, label='$\\mathbf{f}$ non-MLE', alpha=0.4)\n",
    "            \n",
    "        ax[j].grid(True, which=\"both\", linestyle='-')\n",
    "        ax[j].text(3.1, 0.42, r'$N, M = %d, %d$' % (N, M), fontsize=font_legend-3\n",
    "                   # fontsize=FONTSIZE\n",
    "                  ) \n",
    "\n",
    "        ax[j].text(3.1, 0.30, r'$\\nu = %5.1f$' % nu, fontsize=font_legend-3\n",
    "                   # fontsize=FONTSIZE\n",
    "                  ) \n",
    "\n",
    "        ax[j].legend(loc='upper right',fontsize=font_legend-3)\n",
    "        \n",
    "    # hide unused sub-plots\n",
    "    for k in range(j+1, len(ax)):\n",
    "        ax[k].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_image:\n",
    "        plt.savefig('images/h_MLE_nonMLE_f_MLE_f_nonMLE_one_nu%s.png' % str(nu))\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_many_nus_with_model(Bprime, thetamin, thetamax, nu_list, D,\n",
    "                     NBINS,\n",
    "              FONTSIZE=15,\n",
    "              func=None,\n",
    "              fgsize=(10, 6), save_image=False):\n",
    "    \n",
    "    # make room for 6 sub-plots\n",
    "    fig, ax = plt.subplots(nrows=2, \n",
    "                           ncols=2, \n",
    "                           figsize=fgsize)\n",
    "    \n",
    "    outside=''\n",
    "    ALPHA=0.8\n",
    "    TITLE_SIZE=font_legend+1\n",
    "    \n",
    "    # padding\n",
    "    plt.subplots_adjust(hspace=3)\n",
    "    plt.subplots_adjust(wspace=1)#horizontal distance\n",
    "    \n",
    "    # use flatten() to convert a numpy array of \n",
    "    # shape (nrows, ncols) to a 1-d array. \n",
    "    \n",
    "    for nu in nu_list:\n",
    "        \n",
    "        N, M = D\n",
    "        y, bb = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=NBINS,\n",
    "                              MLE=True)\n",
    "    \n",
    "\n",
    "        if nu > 20:\n",
    "            outside = outside + r' ($>$ train data)'\n",
    "        \n",
    "        x = (bb[1:]+bb[:-1])/2\n",
    "        ax[0,0].plot(x, y, lw=2, label=r'$\\nu= %s$ %s' % (str(nu), outside), alpha=ALPHA)\n",
    "        ax[0,0].set_title(r'$\\mathbf{h}$ MLE', fontsize=TITLE_SIZE)\n",
    "        #h is histogram approximation\n",
    "\n",
    "        y_nonMLE, bb_nonMLE = make_hist_data(Bprime,\n",
    "                              thetamin, thetamax,\n",
    "                              nu, N, M,\n",
    "                              nbins=NBINS,\n",
    "                              MLE=False)\n",
    "        \n",
    "        \n",
    "        x_nonMLE = (bb_nonMLE[1:]+bb_nonMLE[:-1])/2\n",
    "\n",
    "        ax[1,0].plot(x_nonMLE, y_nonMLE, lw=2, label=r'$\\nu= %s$ %s' % (str(nu), outside) ,alpha=ALPHA)\n",
    "        ax[1,0].set_title(r'$\\mathbf{h}$ non-MLE',fontsize=TITLE_SIZE)\n",
    "        \n",
    "        if func:\n",
    "            #load the correct dataframe\n",
    "            train_df_MLE = load_train_df(MLE=True)\n",
    "            train_df_nonMLE = load_train_df(MLE=False)\n",
    "            \n",
    "            f_MLE, f_bins_MLE = func(Bprime, train_df_MLE, nu, N, M, MLE=True, nbins=NBINS)\n",
    "            ax[0,1].plot(x, f_MLE, lw=2, label=r'$\\nu= %s$ %s' % (str(nu), outside), alpha=ALPHA)\n",
    "            ax[0,1].set_title(r'$\\mathbf{f}$ MLE',fontsize=TITLE_SIZE)\n",
    "            #f is model approximation\n",
    "            \n",
    "            f_nonMLE, f_bins_nonMLE = func(Bprime, train_df_nonMLE, nu, N, M, MLE=False, nbins=NBINS)\n",
    "            ax[1,1].plot(f_bins_nonMLE, f_nonMLE, lw=2, label=r'$\\nu= %s$ %s' % (str(nu), outside), alpha=ALPHA)\n",
    "            ax[1,1].set_title(r'$\\mathbf{f}$ non-MLE',fontsize=TITLE_SIZE)\n",
    "        \n",
    "        \n",
    "        for i in range(ax.shape[0]):\n",
    "            for j in range(ax.shape[1]):\n",
    "                ax[i,j].set_xlim(thetamin-0.5, thetamax-7)\n",
    "                ax[i,j].set_ylim(0.2, 1.03)\n",
    "                ax[i,j].set_xlabel(r'$\\mathbf{\\theta}$', fontsize=FONTSIZE-2)\n",
    "                ax[i,j].set_ylabel(r'$\\mathbf{E(Z|\\theta, \\nu)}$', fontsize=FONTSIZE-2)\n",
    "                ax[i,j].text(2, 0.5, r'$N, M = %d, %d$' % (N, M), fontsize=font_legend-3\n",
    "                           # fontsize=FONTSIZE\n",
    "                          ) \n",
    "                \n",
    "                ax[i,j].grid(True, which=\"both\", linestyle='-')\n",
    "\n",
    "                ax[i,j].legend(loc='center right',fontsize=font_legend-3)\n",
    "                ax[i,j].patch.set_edgecolor('black')  \n",
    "\n",
    "                ax[i,j].patch.set_linewidth('1')  \n",
    "        # ax[j].text(3.1, 0.30, r'$\\nu = %5.1f$' % nu, fontsize=font_legend-3\n",
    "        #            # fontsize=FONTSIZE\n",
    "        #           ) \n",
    "\n",
    "        \n",
    "        \n",
    "    # hide unused sub-plots\n",
    "#     for k in range(j+1, len(ax)):\n",
    "#         ax[k].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_image:\n",
    "        plt.savefig('images/h_MLE_nonMLE_f_MLE_f_nonMLE_many_nus.png')\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "def get_one_batch(x,  batch_size):\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "\n",
    "    # batch_x.T[-1] = np.random.uniform(0, 1, batch_size)\n",
    "    return batch_x\n",
    "def getwholedata_f_and_lambda(MLE_or_nonMLE, valid=False):\n",
    "    feature_cols = ['theta', 'nu', 'theta_hat', 'N', 'M']\n",
    "    if MLE:\n",
    "        data = pd.read_csv('data/two_parameters_theta_0_20_1000k_Examples_MLE_True.csv', \n",
    "                     # nrows=SUBSAMPLE,\n",
    "                     usecols=feature_cols\n",
    "                    )\n",
    "        \n",
    "    else:\n",
    "        data = pd.read_csv('data/two_parameters_theta_0_20_1000k_Examples_MLE_False.csv', \n",
    "             # nrows=SUBSAMPLE,\n",
    "             usecols=feature_cols\n",
    "            )\n",
    "        \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "    #split the train data (0.8 of whole set) again into 0.8*0.8=0.64 of whole set\n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data  = test_data.reset_index(drop=True)\n",
    "\n",
    "    target='Z'; source = ['theta','nu','theta_hat','N','M']# these are not needed here\n",
    "    train_x = train_data.to_numpy()\n",
    "    test_x = test_data.to_numpy()\n",
    "    #dont return the torch tensors! we want to do operations on them while training\n",
    "        \n",
    "    return train_x, test_x\n",
    "\n",
    "def get_one_batch(x,  batch_size):\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    # batch_x.T[-1] = np.random.uniform(0, 1, batch_size)\n",
    "    return batch_x\n",
    "\n",
    "def RMS(v):\n",
    "    return (torch.mean(v**2))**0.5\n",
    "    \n",
    "def average_quadratic_loss_f_pivot(f, t, df_dnu):\n",
    "    kappa=1.5\n",
    "    #here t will be Z_tilde, f is model_f(x)\n",
    "    return  torch.mean((f - t)**2) - kappa/2 * RMS(df_dnu)\n",
    "\n",
    "\n",
    "def average_quadratic_loss_tildelambda_pivot(lambdatilde, t, dlmbdatilde_dnu):\n",
    "    psi=2\n",
    "    #here t will be lambda_D(N,M), lambda_tilde is model_lambda(x)\n",
    "    return  torch.mean((lambdatilde - t)**2) - psi/2 * RMS(dlambdatilde_dnu)\n",
    "\n",
    "\n",
    "def validate_f(model, avloss, inputs, targets, df_dnu):\n",
    "    # make sure we set evaluation mode so that any training specific\n",
    "    # operations are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and t\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).float()\n",
    "        # remember to reshape!\n",
    "        o = model(x).reshape(t.shape)\n",
    "        #avloss has signature average_quadratic_loss_f_pivot(f, t, df_dnu)\n",
    "    return avloss(o, t, df_dnu)\n",
    "\n",
    "def validate_lambda_tilde(model, avloss, inputs, targets, dlambdatilde_dnu):\n",
    "    # make sure we set evaluation mode so that any training specific\n",
    "    # operations are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and t\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).float()\n",
    "        # remember to reshape!\n",
    "        o = model(x).reshape(t.shape)#this is lambda tilde\n",
    "        #avloss has signaure average_quadratic_loss_tildelambda_pivot(lambdatilde, t, dlmbdatilde_dnu)\n",
    "    return avloss(o, t, dlambdatilde_dnu)\n",
    "\n",
    "def train_pivotal_model_and_lambda(model_f, model_lambda, \n",
    "                                   optimizer_f, optimizer_lambda, \n",
    "                                   avloss_f, avloss_lambda,\n",
    "                                    batch_size, n_iterations, \n",
    "                                   traces_f, traces_lambda, \n",
    "                                      step, window, MLE):\n",
    "    \n",
    "    # to keep track of average losses\n",
    "    xx_F, yy_t_F, yy_v_F, yy_v_avg_F = traces_f\n",
    "    xx_lambda, yy_t_lambda, yy_v_lambda, yy_v_avg_lambda = traces_lambda\n",
    "    \n",
    "    \n",
    "    if MLE:\n",
    "        train_x, test_x = getwholedata_f_and_lambda(MLE_or_nonMLE=True, valid=False)\n",
    "    else:\n",
    "        train_x, test_x = getwholedata_f_and_lambda(MLE_or_nonMLE=False, valid=False)\n",
    "    \n",
    "    #Remember train_x will have columns ['theta','nu','theta_hat','N','M']\n",
    "    n = len(test_x)\n",
    "    print('Iteration vs average loss')\n",
    "    print(\"%10s\\t%10s\\t%10s\" % \\\n",
    "          ('iteration', 'train-set', 'valid-set'))    \n",
    "    for ii in range(n_iterations):\n",
    "        # model.eval()\n",
    "        #Harrison-like Loader\n",
    "        batch_x_train = get_one_batch(train_x,  batch_size)\n",
    "        \n",
    "        #Or Ali's Loader\n",
    "        # batch_x, batch_t = next(training_set_features()), next(training_set_targets())\n",
    "        # batch_x_eval, batch_t_eval = next(evaluation_set_features()), next(evaluation_set_targets())\n",
    "        # x = torch.from_numpy(batch_x).float()\n",
    "        x = batch_x_train\n",
    "        # print('x is leaf: ', x.is_leaf)\n",
    "        # x.retain_grad()\n",
    "        # print('x is leaf after retain: ', x.is_leaf)\n",
    "        # x.requires_grad_(True)\n",
    "        # x.retain_grad()\n",
    "        theta = x[:,0]\n",
    "        nu = x[:,1]\n",
    "        \n",
    "        n = st.poisson.rvs(theta+ nu, size=batch_size)\n",
    "        m = st.poisson.rvs(nu, size=batch_size)\n",
    "        \n",
    "        lambda_gen = lambda_test(theta, n, m, MLE)\n",
    "        \n",
    "        N = x[:,3]\n",
    "        M = x[:,4]\n",
    "        \n",
    "        lambda_D = lambda_test(theta, N, M, MLE)\n",
    "        Z_tilde = (lambda_gen < lambda_D).astype(np.int32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######## take grade of model_f wrt nu\n",
    "        x = torch.tensor(x).float()\n",
    "        x.requires_grad_(True)\n",
    "        f = model_f(x)\n",
    "        f = f.view(-1)\n",
    "        #multiply the model by its ransverse, remember we can only take gradients of scalars\n",
    "        #and f will be a vector before this\n",
    "        f = f @ f.t()\n",
    "        # f = torch.tensor(f, requires_grad=True)\n",
    "        # print('f shape: ', f.shape)\n",
    "        # print('f is leaf: ', f.is_leaf)\n",
    "        \n",
    "        ###################### Get lambda tilde\n",
    "        lambda_tilde = model_lambda(x) * torch.tensor(lambda_D).float()\n",
    "        # take amplitude of lambda_tilde\n",
    "        \n",
    "        lambda_tilde = lambda_tilde.view(-1)\n",
    "        lambda_tilde = lambda_tilde @ lambda_tilde.t()\n",
    "        # f_2 = f**2\n",
    "        # print('f2 shape', f_2.shape)\n",
    "        # nu = torch.autograd.Variable( x[:,1], requires_grad=True)\n",
    "        \n",
    "        # nu=torch.autograd.Variable(x[:,1], requires_grad=True)\n",
    "        # nu=torch.tensor(x[:,1], requires_grad=True)\n",
    "        # print(type(nu))\n",
    "        # nu.retain_grad()\n",
    "        # print('nu shape: ', nu.shape)\n",
    "        # print('nu is leaf: ', nu.is_leaf)\n",
    "        # print('nu type', type(nu))\n",
    "        \n",
    "        \n",
    "        # WE NEED TO RETAIN_GRAD ON NON-LEAF NODES \n",
    "        f.retain_grad()\n",
    "        f.backward(gradient=torch.ones_like(f), retain_graph=True)\n",
    "        df_dx = x.grad\n",
    "        # print('df_dnu =', df_dnu)\n",
    "        # print('df_dx =', df_dx)\n",
    "        # print('df_dx shape :', df_dx.shape)\n",
    "        df_dnu = df_dx[:,1]\n",
    "        # x.grad.zero_()\n",
    "        # print('df_dnu shape: ', df_dnu.shape)\n",
    "        #################### Lambda_tilde gradient ##############################\n",
    "        x.requires_grad_(True)\n",
    "        lambda_tilde.retain_grad()\n",
    "        lambda_tilde.backward(gradient=torch.ones_like(lambda_tilde), retain_graph=True)\n",
    "        dlambda_tilde_dx = x.grad\n",
    "        dlambda_tilde_dnu = dlambda_tilde_dx[:,1]\n",
    "        \n",
    "        #clear the gradient after you take it\n",
    "        x.grad.zero_()\n",
    "        # break        \n",
    "        # with torch.no_grad():\n",
    "        #     x = torch.from_numpy(batch_x).float()\n",
    "        #     t = torch.from_numpy(batch_t).float()   \n",
    "        \n",
    "        ################################################################################\n",
    "        lambda_D = torch.tensor(lambda_D).float()\n",
    "        #lambda_D will be the target for model_lambda\n",
    "        Z_tilde = torch.tensor(Z_tilde).float()\n",
    "        #Z_tilde will be the target for model_fget for model_f\n",
    "        \n",
    "        #target for f\n",
    "        t_f = Z_tilde\n",
    "        t_lambda_tilde = lambda_D\n",
    "        \n",
    "        model_f.train()\n",
    "        outputs_f = model_f(x).reshape(t_f.shape)\n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk_f = avloss_f(outputs_f, t_f, df_dnu)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer_f.zero_grad()       # clear previous gradients\n",
    "        empirical_risk_f.backward()   # compute gradients\n",
    "        # finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer_f.step()            # move one step\n",
    "        \n",
    "        \n",
    "        model_lambda.train()\n",
    "        outputs_lambda_tilde = model_lambda(x).reshape(t_lambda_tilde.shape)\n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk_lambda_tilde = avloss_f(outputs_lambda_tilde, t_lambda_tilde, dlambda_tilde_dnu)\n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer_lambda.zero_grad()       # clear previous gradients\n",
    "        empirical_risk_lambda_tilde.backward()   # compute gradients\n",
    "        # finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer_lambda.step()            # move one step\n",
    "        \n",
    "        \n",
    "#         if ii % step == 0:\n",
    "            \n",
    "#             # this is an example of an x tensor\n",
    "#             # [17.3352, 10.7722,  6.0000,  8.0000],\n",
    "#             #[16.7822, 13.3260,  8.0000,  4.0000],\n",
    "#             #using Harrison-like loader\n",
    "#             batch_x_test = get_one_batch(test_x,  batch_size)\n",
    "#             #validate_f has signature validate_f(model, avloss, inputs, targets, df_dnu)\n",
    "#             acc_t_f = validate_f(model_f, avloss_f, train_x[:n], train_t[:n], df_dnu)\n",
    "#             acc_v = validate(model, avloss, test_x[:n], test_t[:n], df_dnu)\n",
    "            #using Ali's loader\n",
    "            # acc_t = validate(model, avloss, batch_x, batch_t) \n",
    "            # acc_v = validate(model, avloss, batch_x_eval, batch_t_eval)\n",
    "            \n",
    "#             yy_t_F.append(acc_t)\n",
    "#             yy_v_F.append(acc_v)\n",
    "            \n",
    "#             # compute running average for validation data\n",
    "#             len_yy_v_F = len(yy_v_F)\n",
    "#             if   len_yy_v_F < window:\n",
    "#                 yy_v_avg_F.append( yy_v_F[-1] )\n",
    "#             elif len_yy_v_F == window:\n",
    "#                 yy_v_avg_F.append( sum(yy_v_F) / window )\n",
    "#             else:\n",
    "#                 acc_v_avg  = yy_v_avg_F[-1] * window\n",
    "#                 acc_v_avg += yy_v_F[-1] - yy_v_F[-window-1]\n",
    "#                 yy_v_avg_F.append(acc_v_avg / window)\n",
    "                        \n",
    "#             if len(xx_F) < 1:\n",
    "#                 xx_F.append(0)\n",
    "#                 print(\"%10d\\t%10.6f\\t%10.6f\" % \\\n",
    "#                       (xx_F[-1], yy_t_F[-1], yy_v_F[-1]))\n",
    "#             else:\n",
    "#                 xx_F.append(xx_F[-1] + step)\n",
    "                    \n",
    "#                 print(\"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\" % \\\n",
    "#                           (xx_F[-1], yy_t_F[-1], yy_v_F[-1], yy_v_avg_F[-1]), \n",
    "#                       end='')\n",
    "            \n",
    "    print()      \n",
    "    return (xx_F, yy_t_F, yy_v_F, yy_v_avg_F), (xx_lambda, yy_t_lambda, yy_v_lambda, yy_v_avg_lambda)\n",
    "\n",
    "def make_D(train_df):\n",
    "    Nmin = train_df['N'].min()\n",
    "    Nmax = train_df['N'].max()\n",
    "    Mmin = train_df['M'].min()\n",
    "    Mmax = train_df['M'].max()\n",
    "    D = [ (N, M) for N in range(Nmin, Nmax) for M in range(Mmin, Mmax)]\n",
    "    return np.array(D)[[0, 10, 15, 20, 40]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae9d4f-9d3d-4d3c-aaf4-6b1f43bff2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
