{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cb77096-2352-4a60-b6dc-0534d87ee583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3b70d-895e-4b92-a8f9-0525383c863b",
   "metadata": {},
   "source": [
    "\n",
    "In particle physics, the most important experiment is a counting experiment, represented by a Poisson probability model, where $N$ is the observed count and $\\theta$ is a nuissance parameter, hence the probability model is $P(N|\\theta) = \\text{Poisson}(N,\\theta)$.\n",
    "where the most important parameter is the cross section $\\sigma$, which is related to the mean event count \n",
    "\n",
    "$$\\mu = \\sigma \\mathcal{L} +b$$\n",
    "\n",
    "(more fully it is $\\mu = \\varepsilon \\sigma \\mathcal{L} +b$ where $\\varepsilon = \\prod_i \\varepsilon_i$ is the product of all the efficiencies for the signal). Here the interesting parameter is only the cross section, whereas $\\mathcal{L}, \\ b$ are nuissance parameters. In a Bayesian context, one could eliminate the nuissance parameters by marginalization, i.e. by integrating the probability with respect to the nuissance parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4c213-561e-4581-8e0c-a0208b1881cc",
   "metadata": {},
   "source": [
    "$$ P(N, \\mathcal{L}, b| \\sigma, \\mathcal{L}_{truee}, b_{true}) =L(\\sigma, \\mathcal{L}_{truee}, b_{true})= \\frac{e^{-(\\sigma \\mathcal{L} +b)}(\\sigma \\mathcal{L} +b)^N}{N !} \\ Gamma(\\mathcal{L}_{true}) \\ Gamma(b_{true})$$\n",
    "\n",
    "Therefore we have 3 pieces of data:\n",
    "\n",
    "* Observed count $N$\n",
    "* the estimated luminosity\n",
    "* The estimated background\n",
    "\n",
    "We also have 3 parameters:\n",
    "* The cross section $\\sigma$\n",
    "* The true luminosity $\\mathcal{L}$\n",
    "* The true background $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c799654-f35c-4437-b6ee-025f1a6f2cbf",
   "metadata": {},
   "source": [
    "We want to construct a test to that the probability of making a type I error (rejecting the null hypothesis when it is true) is bounded, it cannot be larger than $\\alpha$. This is done by defining a critical region $R(D)$ where $D$ is the observed data, which is composed of all the values of $\\theta$ that are not rejected by the test $\\delta$\n",
    "\n",
    "$$ R(D) = \\{ \\theta_0 \\in \\Theta \\mid \\text{test } \\delta_{\\theta_o} \\text{ does not reject the null hypothesis} \\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30157296-6547-4f09-b5a9-6271be536326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2276c10-143d-4413-b9a1-3074a9ff559a",
   "metadata": {},
   "source": [
    "# Starting Simple: Using Algorithm 2 to calculate the $p$-value of a Poisson distribution.\n",
    "\n",
    "We start with the very simple likelihood \n",
    "$$L(\\theta) = \\frac{e^{-\\theta} \\theta^N}{N !}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188e5602-20c8-450b-9b82-c0fefce42c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of B:  10000\n",
      "The observed signal signal D or N:  10\n",
      "The observed luminosity:  30\n"
     ]
    }
   ],
   "source": [
    "Bprime=10000\n",
    "D = 10\n",
    "L_obs=30 \n",
    "#b= mean background\n",
    "print('The size of B: ', Bprime)\n",
    "print('The observed signal signal D or N: ', D)\n",
    "print('The observed luminosity: ', L_obs)\n",
    "# print('The observed background'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8020d4-7f06-4a2b-9283-1cbf24f62c0a",
   "metadata": {},
   "source": [
    "Note that $D$ is only a constant and appeard only in the calculation of the test statistic $\\lambda (D, \\theta_0)$\n",
    "\n",
    "Test statistic could be the likelihood ratio, which is the ratio of the likelihood to the profiled likelihood (likelihood computed at an MLE estimate of one of the parameters), or $t=-2log (this ratio)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b3745-9243-468d-af1f-1fc81c2ec981",
   "metadata": {},
   "source": [
    "### Test statistic $\\lambda$\n",
    "\n",
    "We have several options for this statistic, such as the likelihood ratio, etc. as is shown by Ann Lee's paper. In our case we'd probably like to use the likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a68fbb0-9452-4683-bed5-1aec416536f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambd(D, theta, thetahat=1):#test statistic\n",
    "    L_num = st.norm.pdf(D, loc= theta, scale=1)#the gaussian pdf of D counts\n",
    "    return L_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd5741-7b5c-4a6a-aef8-df00af5bd552",
   "metadata": {},
   "source": [
    "For our simple example we draw a single $X ~ F_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7208fa75-3058-4dbe-8dac-b94f967ec2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T=[[theta_i],[Z_i]]\n",
    "T = [[],[]]\n",
    "for i in range(Bprime):\n",
    "    theta = round(np.random.normal(10))#draw a count theta from a radom poisson prior, it has to be count because its an input to a poisson. This prior should also be close to the cound D\n",
    "    X_mean = np.random.poisson(lam=theta) #draw count samples randomly from a poisson distribution\n",
    "    lam_true = lambd(D, theta)\n",
    "    lam_i = lambd(X_mean, theta)\n",
    "    if lam_i < lam_true:\n",
    "        Z_i=1\n",
    "    else:\n",
    "        Z_i=0\n",
    "    T[0].append(theta)\n",
    "    T[1].append(Z_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212c131-d472-4f11-aded-54c91ea8d7aa",
   "metadata": {},
   "source": [
    "$\\widehat{\\mathbb{E}}[Z \\mid \\theta] = \\frac{N_{Z=1}}{N_{Z, \\ total}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2e1b04-ef21-4606-b585-0f77fc81e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_hat(T):\n",
    "    \"\"\"The expectation value of Z as a relative frequency, this should equal p_hat, the learned parameterized distribution at a given theta\"\"\"\n",
    "    num = np.array(T[1]).sum()\n",
    "    den = Bprime\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf839d13-1674-475f-8150-728159f0e7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6959"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_hat(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe930c-958a-47e1-a374-d0c00181e809",
   "metadata": {},
   "source": [
    "The actual p-value is $p = \\int P(N|\\theta) d\\theta$ or in our case $p=\\sum_{k=D}^{\\infty} \\text{Poisson}(k|\\theta) = scipy.special.gammainc(D, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965af919-e478-490e-bf34-7108bbcee516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_calculated(theta):\n",
    "    return sp.special.gammainc(D, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb3ddd4-55e1-4652-baa9-6a95aa4b609c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6594893575343392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p_calculated(theta = round(np.random.normal(10))); p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e97f2f4-04af-44f4-8ead-67be9b9c5007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.185435114067817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.gamma(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce52de-413c-4208-8903-5fc6a5e4b1ce",
   "metadata": {},
   "source": [
    "train_dataset[0]Now that we've built up the dataset, we now need to learn the function $\\hat{p}(D;\\theta)=\\hat{p}(\\theta)$ which is the output of a machine learning regression model, where the training data are $\\vec{\\theta}, \\vec{Z}$ so that the target is $Z$ and the (input) features is $\\theta$, so that the NN model's only parameter is $\\theta$, not $D$ because it's just a fixed constant.\n",
    "## Pytorch Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "868640c3-37f4-4f51-af06-1896aee55c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = np.array(T[0]), np.array(T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bad899b-b6c6-4408-8c53-f1531cfeb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]] [[10]\n",
      " [ 9]\n",
      " [ 8]\n",
      " ...\n",
      " [12]\n",
      " [11]\n",
      " [ 9]]\n"
     ]
    }
   ],
   "source": [
    "ntargets = 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, \n",
    "                                                                      targets, \n",
    "                                                                      stratify=targets)\n",
    "#Reshape the targets to have shape (something, 1)\n",
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "print(test_targets, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "727bdcae-7659-4d8f-89b5-4ac654a26dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0368891758456205e-16, 1.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()#this is always recommended for logistic regression\n",
    "train_data= sc.fit_transform(train_data)\n",
    "test_data = sc.transform(test_data)\n",
    "train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e61d6cc3-3580-4d96-8dad-2d4044b11067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This takes the index for the data and target and gives dictionary of tensors of data and targets.\\n    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\\n where train and test_dataset are np array that are reshaped to (-1,1)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mymodels #this library can be found at https://github.com/AliAlkadhim/MyMLFramework/blob/main/mymodels.py\n",
    "mymodels.CustomDataset.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cad3da38-6e59-4062-bf98-9914afb32d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0.9755]), 'y': tensor([1.])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mymodels import CustomDataset\n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af9efe5-0ca6-4383-a890-cf9ea891a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=10, \n",
    "                                           num_workers=2, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02531fe-3ba4-4425-a2ac-7c70ed5bb01c",
   "metadata": {},
   "source": [
    "Compare with Directly computing the $p$-value for the Poisson distribution $PoisS(D|\\lambda=\\theta_0)$. So say we use counts $D=0....20$, then we have 20 jobs running in parallel, each working on a different value of $D$. It might be worth generalizing it so that the model is a parameterized function of both $\\theta$ and $D$.\n",
    "\n",
    "For our case, the goal is to generalize step 3, where we have 3 parameters as opposed to 1, $\\theta =\\{\\sigma, \\mathcal{L}, b \\}$ so that we'd have priors for each of these parameters, so at the end, at a fixed $D$, we'd have the output being the p-value being a function of all 3 $\\hat{p}(D; \\sigma, \\mathcal{L}, b)$. Then we can use section 3.4 to construct the confidence interval for the cross section that properly takes into account the two nuissance parameters $\\mathcal{L}, b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d9ce303-2b53-475b-9baf-0a5b8e665d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8de18693-6e71-418e-aa75-a9d896e26853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=5, \n",
    "               hidden_size=128, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3c5fbca-f306-42ba-874b-5615d1e3e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def loss_fun(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "            return outputs\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bc2639f-6e67-47d6-9d6f-c5c2ef552ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "best_loss = np.inf\n",
    "early_stopping_iter = 10\n",
    "early_stopping_counter = 0\n",
    "EPOCHS=22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3c8e579-b7d5-454c-bdd4-17fff48597c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 0.000408847, Test Loss: 0.00105222\n",
      "Epoch : 1         , Training Loss: 0.00028409, Test Loss: 0.000915958\n",
      "Epoch : 2         , Training Loss: 0.000286773, Test Loss: 0.000869398\n",
      "Epoch : 3         , Training Loss: 0.000402892, Test Loss: 0.00144139\n",
      "Epoch : 4         , Training Loss: 0.000307841, Test Loss: 0.000986041\n",
      "Epoch : 5         , Training Loss: 0.00034197, Test Loss: 0.00076567\n",
      "Epoch : 6         , Training Loss: 0.000310439, Test Loss: 0.000989265\n",
      "Epoch : 7         , Training Loss: 0.000297197, Test Loss: 0.000813849\n",
      "Epoch : 8         , Training Loss: 0.000384276, Test Loss: 0.00094127\n",
      "Epoch : 9         , Training Loss: 0.000251631, Test Loss: 0.000587939\n",
      "Epoch : 10        , Training Loss: 0.000477164, Test Loss: 0.000555949\n",
      "Epoch : 11        , Training Loss: 0.000343835, Test Loss: 0.000746557\n",
      "Epoch : 12        , Training Loss: 0.000372936, Test Loss: 0.000922841\n",
      "Epoch : 13        , Training Loss: 0.000325631, Test Loss: 0.000758718\n",
      "Epoch : 14        , Training Loss: 0.000376629, Test Loss: 0.000659325\n",
      "Epoch : 15        , Training Loss: 0.000289744, Test Loss: 0.000861  \n",
      "Epoch : 16        , Training Loss: 0.000258172, Test Loss: 0.000649499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = eng.train(train_loader)\n",
    "    test_loss = eng.train(test_loader)\n",
    "    print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "    #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        \n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    if early_stopping_counter > early_stopping_iter:\n",
    "        #if we are not improving for 10 iterations then break the loop\n",
    "        #we could save best model here\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fc537f3-02e9-4e0c-9cbc-5fca8d0d2d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5863],\n",
       "         [0.3795],\n",
       "         [0.2159],\n",
       "         [0.3967],\n",
       "         [0.7747],\n",
       "         [0.7936],\n",
       "         [0.6855],\n",
       "         [0.7426],\n",
       "         [0.3960],\n",
       "         [0.7034]]),\n",
       " tensor([[0.5861],\n",
       "         [0.8277],\n",
       "         [0.7988],\n",
       "         [0.6008],\n",
       "         [0.8735],\n",
       "         [0.4286],\n",
       "         [0.6666],\n",
       "         [0.6455],\n",
       "         [0.6496],\n",
       "         [0.0751]])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "labels = []\n",
    "accuracies = []\n",
    "\n",
    "#evaluate\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data_cp = copy.deepcopy(data)\n",
    "        \n",
    "        xtest = data_cp[\"x\"]\n",
    "        ytest = data_cp[\"y\"]\n",
    "        output = model(xtest)\n",
    "        labels.append(ytest)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        y_predicted_cls = output.round()\n",
    "        acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])#bumber of correct predictions/sizeofytest\n",
    "        #accuracies.append(acc.numpy())\n",
    "        #print(f'accuracy: {acc.item():.4f}')\n",
    "        \n",
    "        del data_cp\n",
    "\n",
    "#     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "#     print(f'accuracy: {acc.item():.4f}')\n",
    "outputs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "129f98e0-2280-4e18-ab29-336489821710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58625966 0.3795342  0.21585454 ... 0.85125905 0.8080859  0.63989043] [0. 1. 0. ... 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "LABELS = torch.cat(labels).view(-1).numpy()\n",
    "print(OUTPUTS, LABELS)\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "plt.hist(OUTPUTS, bins=50, density=True, label = \"Outputs $\\hat{y}$\")\n",
    "plt.hist(LABELS, bins=50,density=True, label = \"Labels $y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9db500b-a11e-4eac-be17-92706d8e6163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73431224 0.44496882 0.18356903 ... 0.70291984 0.65752524 0.44496882]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda582bf490>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_prob(model, xx):\n",
    "    # convert from numpy array to a torch tensor of type float\n",
    "    \"\"\"Gives P(y=1|x)\"\"\"\n",
    "    x = torch.from_numpy(xx).float()#.to(device)\n",
    "\n",
    "    # compute p(1|x)\n",
    "    model.eval() # evaluation mode\n",
    "    p = model(x)#.to(device)\n",
    "\n",
    "    # squeeze() removes extraneous dimensions\n",
    "    p = p.squeeze()\n",
    "\n",
    "    # detach().numpy() converts back to a numpy array\n",
    "    p = p.detach().cpu().numpy()\n",
    "    return p\n",
    "p = compute_prob(model, test_data)\n",
    "print(p)\n",
    "plt.hist(p[p>0.5], bins=50, color='g',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 1$')\n",
    "plt.hist(p[p<0.5], bins=50, color='r',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 0$')\n",
    "plt.xlabel('$p(y=1|x)$', fontsize=13)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52abc0-cb90-44a4-8ebc-d5ea084a41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm2(D=2, theta_0):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return actual_p_value, regressed_p_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
