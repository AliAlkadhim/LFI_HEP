{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb77096-2352-4a60-b6dc-0534d87ee583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3b70d-895e-4b92-a8f9-0525383c863b",
   "metadata": {},
   "source": [
    "\n",
    "In particle physics, the most important experiment is a counting experiment, represented by a Poisson probability model, where $N$ is the observed count and $\\theta$ is a nuissance parameter, hence the probability model is $P(N|\\theta) = \\text{Poisson}(N,\\theta)$.\n",
    "where the most important parameter is the cross section $\\sigma$, which is related to the mean event count \n",
    "\n",
    "$$N = \\sigma \\mathcal{L} +b$$\n",
    "\n",
    "\n",
    "Suppose we have the measured background mean $B$ and the estimated background mean $b$, then $P(B|b) = L(b) =\\frac{e^{-b}b^B}{B!}$ so now the posterior, using Baye's rule is $P(b|B) = \\frac{P(B|b)P(b)}{P(B)}$ and if we assume a flat prior for $b$, then it corresponds to an inverse Poisson or Gamma distribution, so $P(b|B) =  \\frac{e^{-b}b^B}{B!} Gamma(b)$. The same argument holds for if we include the signal and other parameters.\n",
    "\n",
    "(more fully it is $\\mu = \\varepsilon \\sigma \\mathcal{L} +b$ where $\\varepsilon = \\prod_i \\varepsilon_i$ is the product of all the efficiencies for the signal). Here the interesting parameter is only the cross section, whereas $\\mathcal{L}, \\ b$ are nuissance parameters. In a Bayesian context, one could eliminate the nuissance parameters by marginalization, i.e. by integrating the probability with respect to the nuissance parameters.\n",
    "\n",
    "Therefore our probability model is \n",
    "\n",
    "$$P(\\text{observed}|\\text{parameters}) = P(N, \\mathcal{L}_{\\text{measured}}| \\sigma, \\mathcal{L}, b)$$\n",
    "\n",
    "or  $P(N, \\mathcal{L}_{\\text{measured}}, B| \\sigma, \\mathcal{L}, b)$ if the background is also measured, where $N$ and $\\mathcal{L}_\\text{measured}$ are measured quantities and $\\sigma, \\mathcal{L}, b$ are parameters, of which $\\mathcal{L},b$ are nuissance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4c213-561e-4581-8e0c-a0208b1881cc",
   "metadata": {},
   "source": [
    "$$ P(N, \\mathcal{L}_{\\text{measured}} | \\sigma, \\mathcal{L}, b) =L(\\sigma, \\mathcal{L}, b)= \\frac{e^{-(\\sigma \\mathcal{L} +b)}(\\sigma \\mathcal{L} +b)^N}{N !} \\ Gamma(\\mathcal{L_{\\text{measured}}}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c799654-f35c-4437-b6ee-025f1a6f2cbf",
   "metadata": {},
   "source": [
    "We want to construct a test to that the probability of making a type I error (rejecting the null hypothesis when it is true) is bounded, it cannot be larger than $\\alpha$. This is done by defining a critical region $R(D)$ where $D$ is the observed data, which is composed of all the values of $\\theta$ that are not rejected by the test $\\delta$\n",
    "\n",
    "$$ R(D) = \\{ \\theta_0 \\in \\Theta \\mid \\text{test } \\delta_{\\theta_o} \\text{ does not reject the null hypothesis} \\}$$\n",
    "\n",
    "We want to collect all these $\\theta_0$ values in $R\\{ D \\}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30157296-6547-4f09-b5a9-6271be536326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2276c10-143d-4413-b9a1-3074a9ff559a",
   "metadata": {},
   "source": [
    "# Starting Simple: Using Algorithm 2 to calculate the $p$-value of a Poisson distribution.\n",
    "\n",
    "We start with the very simple likelihood \n",
    "$$L(\\theta) = \\frac{e^{-\\theta} \\theta^N}{N !}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188e5602-20c8-450b-9b82-c0fefce42c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of B:  1000000\n",
      "The observed signal signal N (or bold X in the paper):  9\n",
      "The observed luminosity:  30\n"
     ]
    }
   ],
   "source": [
    "Bprime=1000000\n",
    "D = 9 #this does not work for large values of D\n",
    "L_obs=30 \n",
    "#b= mean background\n",
    "print('The size of B: ', Bprime)\n",
    "print('The observed signal signal N (or bold X in the paper): ', D)\n",
    "print('The observed luminosity: ', L_obs)\n",
    "# print('The observed background'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8020d4-7f06-4a2b-9283-1cbf24f62c0a",
   "metadata": {},
   "source": [
    "Note that $D$ is only a constant and appeard only in the calculation of the test statistic $\\lambda (D, \\theta_0)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b3745-9243-468d-af1f-1fc81c2ec981",
   "metadata": {},
   "source": [
    "### Test statistic $\\lambda (X, \\theta_0)$\n",
    "\n",
    "Test statistic could be the likelihood ratio, which is the ratio of the likelihood to the profiled likelihood (likelihood computed at an MLE estimate of one of the parameters), or $t=-2log (this ratio)$\n",
    "\n",
    "We have several options for this statistic, such as the likelihood ratio, etc. as is shown by Ann Lee's paper. In our case we'd probably like to use the likelihood ratio. For one parameter, $\\hat{\\theta} = N$ so $\\lambda = -2 \\log \\frac{L(\\theta)}{L(N)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a68fbb0-9452-4683-bed5-1aec416536f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(X, theta):\n",
    "    \"\"\"likelihood with one parameter\"\"\"\n",
    "    return st.poisson.pmf(X, mu=theta)\n",
    "    \n",
    "def labd_one_param(X, theta):\n",
    "    num = L(X, theta)\n",
    "    den = L(X, D)\n",
    "    return -2 * np.log(num/den)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd5741-7b5c-4a6a-aef8-df00af5bd552",
   "metadata": {},
   "source": [
    "For our simple example we draw a single $X ~ F_{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7208fa75-3058-4dbe-8dac-b94f967ec2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T=[[theta_i],[Z_i]]\n",
    "def generate_training_data(Bprime, D):\n",
    "    \n",
    "    T = [[],[]]\n",
    "    for i in range(Bprime):\n",
    "        theta = st.expon.rvs() #sample theta from an exponential distribution\n",
    "        #theta has to be positive because its an input to a poisson. This prior should also be close to the cound D\n",
    "\n",
    "        N = np.random.poisson(lam=theta) #draw count samples randomly from a poisson distribution\n",
    "        #this X is really N\n",
    "        lam_observed = labd_one_param(X=D, theta=theta)#\n",
    "        lam_i = labd_one_param(X=X, theta=theta)\n",
    "        if lam_i < lam_true:\n",
    "            Z_i=1\n",
    "        else:\n",
    "            Z_i=0\n",
    "        T[0].append(theta)\n",
    "        T[1].append(Z_i)\n",
    "        \n",
    "        return np.array(T[0]), np.array(T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d5db83-9d26-4fa7-9f23-52cbfde13124",
   "metadata": {},
   "source": [
    "For a one-parameter problem the statistic is the count itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdd9aca-7b49-4516-846e-b67d5f0edf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_one_parameter(Bprime, D, save_data=False):\n",
    "    #T=[[theta_i],[Z_i]]\n",
    "    T = [[],[]]\n",
    "    for i in range(Bprime):\n",
    "        theta = st.expon.rvs() #sample theta from an exponential distribution\n",
    "        #theta has to be positive because its an input to a poisson. This prior should also be close to the cound D\n",
    "\n",
    "        N = np.random.poisson(lam=theta) #draw count samples randomly from a poisson distribution\n",
    "        #this X is really N\n",
    "\n",
    "        if D < N:\n",
    "            Z_i=1\n",
    "        else:\n",
    "            Z_i=0\n",
    "        T[0].append(theta)\n",
    "        T[1].append(Z_i)\n",
    "        \n",
    "    if save_data:\n",
    "        Training_data_1_param = {'theta' : T[0], 'Z' : T[1]}\n",
    "        Training_data_1_param = pd.DataFrame.from_dict(Training_data_1_param)\n",
    "        Training_data_1_param.to_csv('data/Training_data_1_param_1M.csv')\n",
    "        \n",
    "    return np.array(T[0]), np.array(T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212c131-d472-4f11-aded-54c91ea8d7aa",
   "metadata": {},
   "source": [
    "$\\widehat{\\mathbb{E}}[Z \\mid \\theta] = \\frac{N_{Z=1}}{N_{Z, \\ total}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f83ff3-b91b-4e9d-8b4a-6b5d80c9edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, Z = generate_training_data_one_parameter(Bprime=1000000, D=9, save_data=True)\n",
    "np.sum(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf9162-91fe-4990-987a-35368fed35ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
