{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770c02da-95fb-4fa6-820c-5fe9255b6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c997ed2c-9ec8-40bc-ad20-3ddecfb2923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of B:  1000000\n",
      "The observed signal signal N (or bold X in the paper):  9\n",
      "The observed luminosity:  30\n"
     ]
    }
   ],
   "source": [
    "%run Start.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcbd981-548b-420c-a919-4dd7a3be121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.72912955, 6.63002091, 0.77043243, ..., 1.5963909 , 0.09124841,\n",
       "        0.78852489]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3213-3dd6-41fd-883a-1b3282e8f0cf",
   "metadata": {},
   "source": [
    "train_dataset[0]Now that we've built up the dataset, we now need to learn the function $\\hat{p}(D;\\theta)=\\hat{p}(\\theta)$ which is the output of a machine learning regression model, where the training data are $\\vec{\\theta}, \\vec{Z}$ so that the target is $Z$ and the (input) features is $\\theta$, so that the NN model's only parameter is $\\theta$, not $D$ because it's just a fixed constant.\n",
    "## Pytorch Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64bcbf42-e0cf-417e-b6b5-5770c23e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = theta, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e4017a-74c5-4e5d-811f-639085c52e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[6.35530651]\n",
      " [0.42486313]\n",
      " [2.40667403]\n",
      " ...\n",
      " [1.32953337]\n",
      " [0.07514038]\n",
      " [3.21311493]]\n"
     ]
    }
   ],
   "source": [
    "ntargets = 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, \n",
    "                                                                      targets, \n",
    "                                                                      stratify=targets)\n",
    "#Reshape the targets to have shape (something, 1)\n",
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "print(test_targets, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206573fa-0766-4c17-b129-4226f64bf17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.821713481661088e-17, 0.9999999999999998)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()#this is always recommended for logistic regression\n",
    "train_data= sc.fit_transform(train_data)\n",
    "test_data = sc.transform(test_data)\n",
    "train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd2fffe-57a5-4a33-a4d6-392a36999ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([-0.4903]), 'y': tensor([0.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4fd943-888f-4d9d-9a38-bb2b2bee0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=10, \n",
    "                                           num_workers=2, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054e9c-84b3-4bd4-aff1-04a9729f3194",
   "metadata": {},
   "source": [
    "Compare with Directly computing the $p$-value for the Poisson distribution $PoisS(D|\\lambda=\\theta_0)$. So say we use counts $D=0....20$, then we have 20 jobs running in parallel, each working on a different value of $D$. It might be worth generalizing it so that the model is a parameterized function of both $\\theta$ and $D$.\n",
    "\n",
    "For our case, the goal is to generalize step 3, where we have 3 parameters as opposed to 1, $\\theta =\\{\\sigma, \\mathcal{L}, b \\}$ so that we'd have priors for each of these parameters, so at the end, at a fixed $D$, we'd have the output being the p-value being a function of all 3 $\\hat{p}(D; \\sigma, \\mathcal{L}, b)$. Then we can use section 3.4 to construct the confidence interval for the cross section that properly takes into account the two nuissance parameters $\\mathcal{L}, b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48dca1f1-d342-4890-929a-c6a7b319caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2c781f-2909-4b30-8d57-b42a9d55ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=5, \n",
    "               hidden_size=128, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f88e06-9deb-41cf-8b6d-c59b85df8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def loss_fun(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "            return outputs\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6771a55f-a87b-40a1-b06e-850000362598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    EPOCHS=22\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.train(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4a5996-36b7-45d8-83b7-8e84894671b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 4.94e-06  , Test Loss: 1.10554e-05\n",
      "Epoch : 1         , Training Loss: 3.0928e-06, Test Loss: 1.00906e-05\n",
      "Epoch : 2         , Training Loss: 3.33791e-06, Test Loss: 8.03783e-06\n",
      "Epoch : 3         , Training Loss: 1.87053e-06, Test Loss: 8.77666e-06\n",
      "Epoch : 4         , Training Loss: 1.90986e-06, Test Loss: 6.15908e-06\n",
      "Epoch : 5         , Training Loss: 1.68039e-06, Test Loss: 7.65657e-06\n",
      "Epoch : 6         , Training Loss: 2.01533e-06, Test Loss: 5.56081e-06\n",
      "Epoch : 7         , Training Loss: 8.81726e-07, Test Loss: 3.34643e-06\n",
      "Epoch : 8         , Training Loss: 9.28174e-07, Test Loss: 3.57436e-06\n",
      "Epoch : 9         , Training Loss: 8.40926e-07, Test Loss: 3.92725e-06\n",
      "Epoch : 10        , Training Loss: 7.51296e-07, Test Loss: 3.21638e-06\n",
      "Epoch : 11        , Training Loss: 8.29418e-07, Test Loss: 2.86642e-06\n",
      "Epoch : 12        , Training Loss: 6.01583e-07, Test Loss: 2.00377e-06\n",
      "Epoch : 13        , Training Loss: 4.97157e-07, Test Loss: 2.43531e-06\n",
      "Epoch : 14        , Training Loss: 8.44933e-07, Test Loss: 1.82906e-06\n",
      "Epoch : 15        , Training Loss: 5.47074e-07, Test Loss: 1.14232e-06\n",
      "Epoch : 16        , Training Loss: 3.98378e-07, Test Loss: 8.01153e-07\n",
      "Epoch : 17        , Training Loss: 7.76219e-07, Test Loss: 9.94372e-07\n",
      "Epoch : 18        , Training Loss: 3.88611e-07, Test Loss: 1.08141e-06\n",
      "Epoch : 19        , Training Loss: 3.02237e-07, Test Loss: 1.1131e-06\n",
      "Epoch : 20        , Training Loss: 1.36154e-07, Test Loss: 1.03886e-06\n",
      "Epoch : 21        , Training Loss: 4.00863e-07, Test Loss: 7.68224e-07\n"
     ]
    }
   ],
   "source": [
    "train(optimizer = torch.optim.Adam(model.parameters(), lr = 0.001),\n",
    "      engine = RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 10,\n",
    "      epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afabaef-dc55-4af6-802f-94e3fe681ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])#bumber of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f697f904-f7d2-4163-981a-547a7147ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [0.01833854 0.04256842 0.04413904 ... 0.03666249 0.05289968 0.01217788]\n",
      "\n",
      "actual labels (targets Z):  [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f39b31-3e62-49be-a0c5-579e9e8f0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.hist(OUTPUTS, bins=50, density=True, label = \"Outputs $\\hat{y}$\")\n",
    "plt.hist(LABELS, bins=50,density=True, label = \"Labels $y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f854ce9e-f2dc-4c34-b13d-e4c718dfeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob(model, xx):\n",
    "    # convert from numpy array to a torch tensor of type float\n",
    "    \"\"\"Gives P(y=1|x)\"\"\"\n",
    "    x = torch.from_numpy(xx).float()\n",
    "    \n",
    "    # compute p(1|x)\n",
    "    model.eval() # evaluation mode\n",
    "    p = model(x)#.to(device)\n",
    "\n",
    "    # squeeze() removes extraneous dimensions\n",
    "    p = p.squeeze()\n",
    "\n",
    "    # detach().numpy() converts back to a numpy array\n",
    "    p = p.detach().numpy()\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2de0e9b3-19d4-45a2-a948-8a5859afae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05493802 0.04800607 0.07690107 ... 0.05490386 0.06811897 0.1500393 ]\n"
     ]
    }
   ],
   "source": [
    "p_hat = compute_prob(model, test_data)\n",
    "print(p_hat)#here test_data is just theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db13245-6fa9-4b6c-86da-ad44ced6c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f947ca8e710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p[p>0.5], bins=50, color='g',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 1$')\n",
    "\n",
    "plt.hist(p[p<0.5], bins=50, color='r',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 0$')\n",
    "plt.xlabel('$p(y=1|x)$', fontsize=13)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbfbe1-a1ee-4762-9c40-b0b5c6329b03",
   "metadata": {},
   "source": [
    "st.expon.rvs(size=len(test_data))The actual p-value is $p = \\int P(N|\\theta) d\\theta$ or in our case $p=\\sum_{k=D+1}^{\\infty} \\text{Poisson}(k|\\theta) = scipy.special.gammainc(D, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f67dd34e-74df-4c70-8feb-da09fae8dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=9\n",
    "def p_calculated(theta):\n",
    "    return sp.special.gammainc(D, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e1b4da8-59b9-4a1c-af8c-ac4fd0dddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_p_values(model):\n",
    "    theta = st.expon.rvs(size=len(test_data))\n",
    "    \n",
    "    p_hat = compute_prob(model, theta)#model evaluated at theta\n",
    "    p_calculated = p_calculated(theta = theta)\n",
    "    \n",
    "    plt.hist(p_hat, label = r'$\\hat{p}$', alpha=0.3)\n",
    "    plt.hist(p_calculated, label='$p$ calculated', alpha=0.3)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7037e065-f805-41c5-b14f-808cd701ea4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3758/3597554309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_p_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3758/3431892804.py\u001b[0m in \u001b[0;36mcompare_p_values\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#model evaluated at theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mp_calculated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_calculated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3758/3437482318.py\u001b[0m in \u001b[0;36mcompute_prob\u001b[0;34m(model, xx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# compute p(1|x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# squeeze() removes extraneous dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3758/195676802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)"
     ]
    }
   ],
   "source": [
    "compare_p_values(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feca878-2070-4522-b88c-ddbb329d37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm2(D=2, theta_0):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return actual_p_value, regressed_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca910987-45ca-4197-9d17-707287948334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_hat(T):\n",
    "    \"\"\"The expectation value of Z as a relative frequency, this should equal p_hat, the learned parameterized distribution at a given theta\"\"\"\n",
    "    num = np.array(T[1]).sum()\n",
    "    den = Bprime\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01507c62-bd5c-4889-b05d-015768d8a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
