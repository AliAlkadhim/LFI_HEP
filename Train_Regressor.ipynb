{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770c02da-95fb-4fa6-820c-5fe9255b6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5685bfc-3ef5-4eea-bf5d-9c20cdfb650a",
   "metadata": {},
   "source": [
    "To generate data, either run the cell below, or open the data from the `/data` directry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c997ed2c-9ec8-40bc-ad20-3ddecfb2923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of B:  1000000\n",
      "The observed signal signal N (or bold X in the paper):  9\n",
      "The observed luminosity:  30\n"
     ]
    }
   ],
   "source": [
    "%run Generate_Training_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebcbd981-548b-420c-a919-4dd7a3be121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25591303, 0.49925543, 0.02589351, ..., 3.12011219, 0.62889287,\n",
       "        0.19194844]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf651dc-d613-4642-a0b5-ba8faefeff76",
   "metadata": {},
   "source": [
    "To load pre-generated data, run the cell bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17f19c30-3c72-4b98-b164-d75f87e3d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>theta</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.255913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.499255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.025894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.222043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.505229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     theta  Z\n",
       "0           0  0.255913  0\n",
       "1           1  0.499255  0\n",
       "2           2  0.025894  0\n",
       "3           3  1.222043  0\n",
       "4           4  1.505229  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_1_param = pd.read_csv('data/Training_data_1_param_1M.csv')\n",
    "training_data_1_param.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6865fe4-9fca-4d56-bd28-026446c28ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array(training_data_1_param.theta)\n",
    "Z = np.array(training_data_1_param.Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3213-3dd6-41fd-883a-1b3282e8f0cf",
   "metadata": {},
   "source": [
    "train_dataset[0]Now that we've built up the dataset, we now need to learn the function $\\hat{p}(D;\\theta)=\\hat{p}(\\theta)$ which is the output of a machine learning regression model, where the training data are $\\vec{\\theta}, \\vec{Z}$ so that the target is $Z$ and the (input) features is $\\theta$, so that the NN model's only parameter is $\\theta$, not $D$ because it's just a fixed constant.\n",
    "## Pytorch Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64bcbf42-e0cf-417e-b6b5-5770c23e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = theta, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11e4017a-74c5-4e5d-811f-639085c52e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[0.16269257]\n",
      " [0.08802537]\n",
      " [2.33957722]\n",
      " ...\n",
      " [1.51793652]\n",
      " [0.70227443]\n",
      " [3.32827901]]\n"
     ]
    }
   ],
   "source": [
    "ntargets = 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, \n",
    "                                                                      targets, \n",
    "                                                                      stratify=targets)\n",
    "#Reshape the targets to have shape (something, 1)\n",
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "print(test_targets, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c96806c-511a-41e8-9b55-d7bb1a710399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (250000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_data), test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "206573fa-0766-4c17-b129-4226f64bf17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.307055370290376e-16, 0.9999999999999996)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()#this is always recommended for logistic regression\n",
    "train_data= sc.fit_transform(train_data)\n",
    "test_data = sc.transform(test_data)\n",
    "train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cd2fffe-57a5-4a33-a4d6-392a36999ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([-0.5400]), 'y': tensor([0.])} <__main__.CustomDataset object at 0x7f46ced684d0>\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "print(train_dataset[0], train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d4fd943-888f-4d9d-9a38-bb2b2bee0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=10, \n",
    "                                           num_workers=2, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054e9c-84b3-4bd4-aff1-04a9729f3194",
   "metadata": {},
   "source": [
    "Compare with Directly computing the $p$-value for the Poisson distribution $PoisS(D|\\lambda=\\theta_0)$. So say we use counts $D=0....20$, then we have 20 jobs running in parallel, each working on a different value of $D$. It might be worth generalizing it so that the model is a parameterized function of both $\\theta$ and $D$.\n",
    "\n",
    "For our case, the goal is to generalize step 3, where we have 3 parameters as opposed to 1, $\\theta =\\{\\sigma, \\mathcal{L}, b \\}$ so that we'd have priors for each of these parameters, so at the end, at a fixed $D$, we'd have the output being the p-value being a function of all 3 $\\hat{p}(D; \\sigma, \\mathcal{L}, b)$. Then we can use section 3.4 to construct the confidence interval for the cross section that properly takes into account the two nuissance parameters $\\mathcal{L}, b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48dca1f1-d342-4890-929a-c6a7b319caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    #inherit from the super classdddddddddddd\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b2c781f-2909-4b30-8d57-b42a9d55ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=5, \n",
    "               hidden_size=128, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71f88e06-9deb-41cf-8b6d-c59b85df8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %writefile training/RegressionEngine.py\n",
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def loss_fun(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()#only optimize weights for the current batch, otherwise it's meaningless!\n",
    "            inputs = data[\"x\"]\n",
    "            targets = data[\"y\"]\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "            return outputs.flatten()\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6771a55f-a87b-40a1-b06e-850000362598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    EPOCHS=22\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.train(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa4a5996-36b7-45d8-83b7-8e84894671b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 2.82598e-06, Test Loss: 8.62881e-06\n",
      "Epoch : 1         , Training Loss: 2.85964e-06, Test Loss: 7.87153e-06\n",
      "Epoch : 2         , Training Loss: 2.56219e-06, Test Loss: 6.61375e-06\n",
      "Epoch : 3         , Training Loss: 2.19087e-06, Test Loss: 5.04304e-06\n",
      "Epoch : 4         , Training Loss: 1.94101e-06, Test Loss: 6.06581e-06\n",
      "Epoch : 5         , Training Loss: 1.26343e-06, Test Loss: 5.1938e-06\n",
      "Epoch : 6         , Training Loss: 1.52966e-06, Test Loss: 4.3996e-06\n",
      "Epoch : 7         , Training Loss: 1.60388e-06, Test Loss: 3.92269e-06\n",
      "Epoch : 8         , Training Loss: 1.45976e-06, Test Loss: 3.39964e-06\n",
      "Epoch : 9         , Training Loss: 1.06087e-06, Test Loss: 2.86479e-06\n",
      "Epoch : 10        , Training Loss: 7.56303e-07, Test Loss: 2.38413e-06\n",
      "Epoch : 11        , Training Loss: 8.04257e-07, Test Loss: 2.12801e-06\n",
      "Epoch : 12        , Training Loss: 7.48355e-07, Test Loss: 2.52085e-06\n",
      "Epoch : 13        , Training Loss: 6.14941e-07, Test Loss: 1.57528e-06\n",
      "Epoch : 14        , Training Loss: 5.40134e-07, Test Loss: 1.37401e-06\n",
      "Epoch : 15        , Training Loss: 4.85074e-07, Test Loss: 1.56327e-06\n",
      "Epoch : 16        , Training Loss: 6.49732e-07, Test Loss: 1.0911e-06\n",
      "Epoch : 17        , Training Loss: 4.75403e-07, Test Loss: 8.81952e-07\n",
      "Epoch : 18        , Training Loss: 3.22699e-07, Test Loss: 8.91715e-07\n",
      "Epoch : 19        , Training Loss: 3.24825e-07, Test Loss: 9.56494e-07\n",
      "Epoch : 20        , Training Loss: 4.08888e-07, Test Loss: 7.01303e-07\n",
      "Epoch : 21        , Training Loss: 3.07244e-07, Test Loss: 9.70479e-07\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train(optimizer, \n",
    "      engine =RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 10,\n",
    "      epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ffe2d65-6034-46e7-a477-86ac3d6ec3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6afabaef-dc55-4af6-802f-94e3fe681ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])#bumber of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS.flatten(), LABELS.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f697f904-f7d2-4163-981a-547a7147ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [0.25560373 0.21802677 0.11195722 ... 0.12836051 0.07702094 0.10614531]\n",
      "\n",
      "actual labels (targets Z):  [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "339bf3e6-a4a3-4f44-939f-e784065dbe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUTS.shape , LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41f39b31-3e62-49be-a0c5-579e9e8f0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.hist(OUTPUTS, bins=50, density=True, label = \"Outputs $\\hat{y}$\")\n",
    "plt.hist(LABELS, bins=50,density=True, label = \"Labels $y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9481a2a8-baaf-4c8f-bd87-a5f489c38bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def calc_phat_from_regressor(model, X):\n",
    "    X = torch.from_numpy(X).float()\n",
    "    \n",
    "    X= Tensor(X)\n",
    "    model.eval()\n",
    "    P_y_equals_1 = model(X)\n",
    "    P_y_equals_1 = P_y_equals_1.squeeze()\n",
    "    return P_y_equals_1.detach().numpy().flatten()#detaches it from the computational history/prevent future computations from being tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff8e957d-425a-4988-b234-d7f8a29f1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Z_equals_1 = calc_phat_from_regressor(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4c45868-5091-4d77-87ab-ee0debc523c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Z_equals_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7a683-9779-4eb4-bf2b-6ed849cedd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(P_Z_equals_1, label='$\\hat{p}(Z=1|x)$')\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3db13245-6fa9-4b6c-86da-ad44ced6c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f062c6d7090>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(P_Z_equals_1[P_Z_equals_1>0.5], bins=50, color='g',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 1$')\n",
    "\n",
    "plt.hist(P_Z_equals_1[P_Z_equals_1<0.5], bins=50, color='r',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 0$')\n",
    "plt.xlabel('$p(y=1|x)$', fontsize=13)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbfbe1-a1ee-4762-9c40-b0b5c6329b03",
   "metadata": {},
   "source": [
    "st.expon.rvs(size=len(test_data))\n",
    "\n",
    "### The actual p-value is $p = \\int P(N|\\theta) d\\theta$ or in our case $p=\\sum_{k=D+1}^{\\infty} \\text{Poisson}(k|\\theta) = scipy.special.gammainc(D, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f67dd34e-74df-4c70-8feb-da09fae8dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=9\n",
    "def p_calculated(theta):\n",
    "    #for the poissons, the calculated p-value is the gamma function\n",
    "    p_computed = sp.special.gammainc(D, theta)\n",
    "    p_computed = np.array(p_computed)\n",
    "    return p_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "167697f5-8745-411a-ac8f-f1c780c2fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_calc = p_calculated(10); type(p_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8341aec7-71ac-4686-bfb4-3564d62f146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(st.expon.rvs(size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5334770b-feec-4665-accf-67e0fed6a1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, numpy.ndarray)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22eb985b-1b10-4c21-b344-72336c914a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "theta_for_test = st.expon.rvs(size=len(test_data)) \n",
    "#th = torch.from_numpy(th)\n",
    "print(theta_for_test.shape, type(th))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb20ea-b999-40db-bfe3-55ec7e151ac0",
   "metadata": {},
   "source": [
    "I might need to write a validation script with no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48be4d8a-483f-424f-abe6-4583999f5a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1000000 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31338/2894391258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_phat_from_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mp_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31338/3688356568.py\u001b[0m in \u001b[0;36mcalc_phat_from_regressor\u001b[0;34m(model, X)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mP_y_equals_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mP_y_equals_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP_y_equals_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mP_y_equals_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#detaches it from the computational history/prevent future computations from being tracked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31338/2491814721.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1000000 and 1x128)"
     ]
    }
   ],
   "source": [
    "p_hat = calc_phat_from_regressor(model=model, X=theta); p_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e1b4da8-59b9-4a1c-af8c-ac4fd0dddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_p_values(model):\n",
    "    theta = st.expon.rvs(size=len(test_data))\n",
    "    \n",
    "    p_hat = compute_prob(model, theta)#model evaluated at theta\n",
    "    p_calculated = [p_calculated(theta = theta[i]) for i in range(len(theta))]\n",
    "    \n",
    "    plt.hist(p_hat, label = r'$\\hat{p}$', alpha=0.3)\n",
    "    plt.hist(p_calculated, label='$p$ calculated', alpha=0.3)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7037e065-f805-41c5-b14f-808cd701ea4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12983/3597554309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_p_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12983/2637657473.py\u001b[0m in \u001b[0;36mcompare_p_values\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#model evaluated at theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mp_calculated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_calculated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12983/3437482318.py\u001b[0m in \u001b[0;36mcompute_prob\u001b[0;34m(model, xx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# compute p(1|x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# squeeze() removes extraneous dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12983/195676802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)"
     ]
    }
   ],
   "source": [
    "compare_p_values(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feca878-2070-4522-b88c-ddbb329d37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm2(D=2, theta_0):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return actual_p_value, regressed_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca910987-45ca-4197-9d17-707287948334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_hat(T):\n",
    "    \"\"\"The expectation value of Z as a relative frequency, this should equal p_hat, the learned parameterized distribution at a given theta\"\"\"\n",
    "    num = np.array(T[1]).sum()\n",
    "    den = Bprime\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01507c62-bd5c-4889-b05d-015768d8a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
