{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770c02da-95fb-4fa6-820c-5fe9255b6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c997ed2c-9ec8-40bc-ad20-3ddecfb2923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of B:  1000000\n",
      "The observed signal signal N (or bold X in the paper):  9\n",
      "The observed luminosity:  30\n"
     ]
    }
   ],
   "source": [
    "%run Generate_Training_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcbd981-548b-420c-a919-4dd7a3be121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.03716309, 0.49266101, 0.41921644, ..., 2.4699644 , 0.05047324,\n",
       "        0.21993042]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d3213-3dd6-41fd-883a-1b3282e8f0cf",
   "metadata": {},
   "source": [
    "train_dataset[0]Now that we've built up the dataset, we now need to learn the function $\\hat{p}(D;\\theta)=\\hat{p}(\\theta)$ which is the output of a machine learning regression model, where the training data are $\\vec{\\theta}, \\vec{Z}$ so that the target is $Z$ and the (input) features is $\\theta$, so that the NN model's only parameter is $\\theta$, not $D$ because it's just a fixed constant.\n",
    "## Pytorch Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64bcbf42-e0cf-417e-b6b5-5770c23e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = theta, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e4017a-74c5-4e5d-811f-639085c52e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[2.63924849e-01]\n",
      " [1.30358175e+00]\n",
      " [1.52100683e-03]\n",
      " ...\n",
      " [2.23957357e+00]\n",
      " [1.72069363e-01]\n",
      " [1.44263224e+00]]\n"
     ]
    }
   ],
   "source": [
    "ntargets = 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(data, \n",
    "                                                                      targets, \n",
    "                                                                      stratify=targets)\n",
    "#Reshape the targets to have shape (something, 1)\n",
    "train_targets = train_targets.reshape(-1,1)\n",
    "test_targets = test_targets.reshape(-1,1)\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "print(test_targets, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c96806c-511a-41e8-9b55-d7bb1a710399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(test_data), test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206573fa-0766-4c17-b129-4226f64bf17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.802262457010026e-17, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()#this is always recommended for logistic regression\n",
    "train_data= sc.fit_transform(train_data)\n",
    "test_data = sc.transform(test_data)\n",
    "train_data.mean(), (train_data.std())**2#check to make sure mean=0, std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd2fffe-57a5-4a33-a4d6-392a36999ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([-0.3112]), 'y': tensor([0.])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomDataset:\n",
    "    \"\"\"This takes the index for the data and target and gives dictionary of tensors of data and targets.\n",
    "    For example we could do train_dataset = CustomDataset(train_data, train_targets); test_dataset = CustomDataset(test_data, test_targets)\n",
    " where train and test_dataset are np arrays that are reshaped to (-1,1).\n",
    " Then train_dataset[0] gives a dictionary of samples \"X\" and targets\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets=targets\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        current_sample = self.data[idx, :]\n",
    "        current_target = self.targets[idx]\n",
    "        return {\"x\": torch.tensor(current_sample, dtype = torch.float),\n",
    "               \"y\": torch.tensor(current_target, dtype= torch.float),\n",
    "               }#this already makes the targets made of one tensor (of one value) each\n",
    "    \n",
    "train_dataset = CustomDataset(train_data, train_targets)\n",
    "test_dataset = CustomDataset(test_data, test_targets)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4fd943-888f-4d9d-9a38-bb2b2bee0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=10, \n",
    "                                           num_workers=2, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054e9c-84b3-4bd4-aff1-04a9729f3194",
   "metadata": {},
   "source": [
    "Compare with Directly computing the $p$-value for the Poisson distribution $PoisS(D|\\lambda=\\theta_0)$. So say we use counts $D=0....20$, then we have 20 jobs running in parallel, each working on a different value of $D$. It might be worth generalizing it so that the model is a parameterized function of both $\\theta$ and $D$.\n",
    "\n",
    "For our case, the goal is to generalize step 3, where we have 3 parameters as opposed to 1, $\\theta =\\{\\sigma, \\mathcal{L}, b \\}$ so that we'd have priors for each of these parameters, so at the end, at a fixed $D$, we'd have the output being the p-value being a function of all 3 $\\hat{p}(D; \\sigma, \\mathcal{L}, b)$. Then we can use section 3.4 to construct the confidence interval for the cross section that properly takes into account the two nuissance parameters $\\mathcal{L}, b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48dca1f1-d342-4890-929a-c6a7b319caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mymodels import RegressionModel\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.ReLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "        \n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2c781f-2909-4b30-8d57-b42a9d55ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Dropout(p=0.3, inplace=False)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Dropout(p=0.3, inplace=False)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Dropout(p=0.3, inplace=False)\n",
      "    (19): ReLU()\n",
      "    (20): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (21): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model =  RegressionModel(nfeatures=train_data.shape[1], \n",
    "               ntargets=1,\n",
    "               nlayers=5, \n",
    "               hidden_size=128, \n",
    "               dropout=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f88e06-9deb-41cf-8b6d-c59b85df8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionEngine:\n",
    "    \"\"\"loss, training and evaluation\"\"\"\n",
    "    def __init__(self, model, optimizer):\n",
    "                 #, device):\n",
    "        self.model = model\n",
    "        #self.device= device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    #the loss function returns the loss function. It is a static method so it doesn't need self\n",
    "    @staticmethod\n",
    "    def loss_fun(targets, outputs):\n",
    "         return nn.MSELoss()(outputs, targets)\n",
    "\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.train()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            final_loss += loss.item()\n",
    "            return final_loss / len(data_loader)\n",
    "\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"the training function: takes the training dataloader\"\"\"\n",
    "        self.model.eval()\n",
    "        final_loss = 0\n",
    "        for data in data_loader:\n",
    "            inputs = data[\"x\"]#.to(self.device)\n",
    "            targets = data[\"y\"]#.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fun(targets, outputs)\n",
    "            final_loss += loss.item()\n",
    "            return outputs\n",
    "            #return final_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6771a55f-a87b-40a1-b06e-850000362598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, engine, early_stopping_iter, epochs):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    eng = RegressionEngine(model=model, optimizer = optimizer)\n",
    "    best_loss = np.inf\n",
    "    #early_stopping_iter = 10\n",
    "    early_stopping_counter = 0\n",
    "    EPOCHS=22\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(train_loader)\n",
    "        test_loss = eng.train(test_loader)\n",
    "        print(\"Epoch : %-10g, Training Loss: %-10g, Test Loss: %-10g\" % (epoch, train_loss, test_loss))\n",
    "        #print(f\"{epoch}, {train_loss}, {test_loss}\")\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            #if we are not improving for 10 iterations then break the loop\n",
    "            #we could save best model here\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa4a5996-36b7-45d8-83b7-8e84894671b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0         , Training Loss: 3.69397e-05, Test Loss: 0.000126544\n",
      "Epoch : 1         , Training Loss: 3.37579e-05, Test Loss: 0.00010228\n",
      "Epoch : 2         , Training Loss: 3.54686e-05, Test Loss: 8.75492e-05\n",
      "Epoch : 3         , Training Loss: 2.72603e-05, Test Loss: 7.55308e-05\n",
      "Epoch : 4         , Training Loss: 2.60621e-05, Test Loss: 7.58234e-05\n",
      "Epoch : 5         , Training Loss: 2.2583e-05, Test Loss: 7.17335e-05\n",
      "Epoch : 6         , Training Loss: 2.03923e-05, Test Loss: 4.53103e-05\n",
      "Epoch : 7         , Training Loss: 1.95032e-05, Test Loss: 4.82579e-05\n",
      "Epoch : 8         , Training Loss: 1.49175e-05, Test Loss: 4.80574e-05\n",
      "Epoch : 9         , Training Loss: 1.63458e-05, Test Loss: 3.94717e-05\n",
      "Epoch : 10        , Training Loss: 1.2898e-05, Test Loss: 3.17249e-05\n",
      "Epoch : 11        , Training Loss: 1.10538e-05, Test Loss: 3.25087e-05\n",
      "Epoch : 12        , Training Loss: 9.59388e-06, Test Loss: 2.78374e-05\n",
      "Epoch : 13        , Training Loss: 7.65477e-06, Test Loss: 2.33502e-05\n",
      "Epoch : 14        , Training Loss: 9.49147e-06, Test Loss: 1.94423e-05\n",
      "Epoch : 15        , Training Loss: 6.84992e-06, Test Loss: 2.68356e-05\n",
      "Epoch : 16        , Training Loss: 6.0891e-06, Test Loss: 1.28847e-05\n",
      "Epoch : 17        , Training Loss: 6.43989e-06, Test Loss: 1.01259e-05\n",
      "Epoch : 18        , Training Loss: 5.46187e-06, Test Loss: 1.46033e-05\n",
      "Epoch : 19        , Training Loss: 4.58945e-06, Test Loss: 1.28958e-05\n",
      "Epoch : 20        , Training Loss: 5.06007e-06, Test Loss: 1.1622e-05\n",
      "Epoch : 21        , Training Loss: 3.81163e-06, Test Loss: 9.17852e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "train(optimizer, engine =RegressionEngine(model=model, optimizer = optimizer),\n",
    "      early_stopping_iter = 10,\n",
    "      epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6afabaef-dc55-4af6-802f-94e3fe681ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    accuracies = []\n",
    "\n",
    "    #evaluate\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data_cp = copy.deepcopy(data)\n",
    "\n",
    "            xtest = data_cp[\"x\"]\n",
    "            ytest = data_cp[\"y\"]\n",
    "            output = model(xtest)\n",
    "            labels.append(ytest)\n",
    "            outputs.append(output)\n",
    "\n",
    "            y_predicted_cls = output.round()\n",
    "            acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])#bumber of correct predictions/sizeofytest\n",
    "            #accuracies.append(acc.numpy())\n",
    "            #print(f'accuracy: {acc.item():.4f}')\n",
    "\n",
    "            del data_cp\n",
    "\n",
    "    #     acc = y_predicted_cls.eq(ytest).sum() / float(ytest.shape[0])\n",
    "    #     print(f'accuracy: {acc.item():.4f}')\n",
    "            \n",
    "    OUTPUTS = torch.cat(outputs).view(-1).numpy()\n",
    "\n",
    "    LABELS = torch.cat(labels).view(-1).numpy()\n",
    "    print('outputs of model: ', OUTPUTS)\n",
    "    print('\\nactual labels (targets Z): ', LABELS)\n",
    "    return OUTPUTS, LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f697f904-f7d2-4163-981a-547a7147ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of model:  [0.13829915 0.07992691 0.16714323 ... 0.0918419  0.21094066 0.107361  ]\n",
      "\n",
      "actual labels (targets Z):  [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "OUTPUTS, LABELS = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "339bf3e6-a4a3-4f44-939f-e784065dbe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUTS.shape , LABELS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9481a2a8-baaf-4c8f-bd87-a5f489c38bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_1(model, X):\n",
    "    X = torch.from_numpy(X).float()\n",
    "    model.eval()\n",
    "    P_y_1 = model(X)\n",
    "    return P_y_1.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff8e957d-425a-4988-b234-d7f8a29f1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_z_1 = calc_prob_1(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb67f5-84bf-418f-a945-fec00182ea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41f39b31-3e62-49be-a0c5-579e9e8f0deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.hist(OUTPUTS, bins=50, density=True, label = \"Outputs $\\hat{y}$\")\n",
    "plt.hist(LABELS, bins=50,density=True, label = \"Labels $y$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f854ce9e-f2dc-4c34-b13d-e4c718dfeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob(model, xx):\n",
    "    # convert from numpy array to a torch tensor of type float\n",
    "    \"\"\"Gives P(y=1|x)\"\"\"\n",
    "    x = torch.from_numpy(xx).float()\n",
    "    \n",
    "    # compute p(1|x)\n",
    "    model.eval() # evaluation mode\n",
    "    p = model(x)#.to(device)\n",
    "\n",
    "    # squeeze() removes extraneous dimensions\n",
    "    p = p.squeeze()\n",
    "\n",
    "    # detach().numpy() converts back to a numpy array\n",
    "    p = p.detach().numpy()\n",
    "    p = np.array(p)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2de0e9b3-19d4-45a2-a948-8a5859afae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21988183 0.2910183  0.2254902  ... 0.2717545  0.22470467 0.29625228] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "p_hat = compute_prob(model, test_data)\n",
    "print(p_hat, type(p_hat))#here test_data is just theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ed7a683-9779-4eb4-bf2b-6ed849cedd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.0000e+00, 9.0000e+00, 9.0000e+00, 2.3000e+01, 2.6000e+01,\n",
       "        3.3000e+01, 1.2362e+04, 6.6390e+03, 4.7900e+03, 1.1030e+03]),\n",
       " array([0.01451825, 0.04771905, 0.08091985, 0.11412064, 0.14732143,\n",
       "        0.18052223, 0.21372302, 0.24692382, 0.2801246 , 0.3133254 ,\n",
       "        0.3465262 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_hat, label='\\hat{p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3db13245-6fa9-4b6c-86da-ad44ced6c125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f947ca8e710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p[p>0.5], bins=50, color='g',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 1$')\n",
    "\n",
    "plt.hist(p[p<0.5], bins=50, color='r',\n",
    "            histtype='stepfilled',\n",
    "            alpha=0.3,label = '$t = 0$')\n",
    "plt.xlabel('$p(y=1|x)$', fontsize=13)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cbfbe1-a1ee-4762-9c40-b0b5c6329b03",
   "metadata": {},
   "source": [
    "st.expon.rvs(size=len(test_data))The actual p-value is $p = \\int P(N|\\theta) d\\theta$ or in our case $p=\\sum_{k=D+1}^{\\infty} \\text{Poisson}(k|\\theta) = scipy.special.gammainc(D, \\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f67dd34e-74df-4c70-8feb-da09fae8dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=9\n",
    "def p_calculated(theta):\n",
    "    p_computed = sp.special.gammainc(D, theta)\n",
    "    p_computed = np.array(p_computed)\n",
    "    return p_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "167697f5-8745-411a-ac8f-f1c780c2fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_calc = p_calculated(10); type(p_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8341aec7-71ac-4686-bfb4-3564d62f146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(st.expon.rvs(size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22eb985b-1b10-4c21-b344-72336c914a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = st.expon.rvs(size=len(test_data)); th.shape; type(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48be4d8a-483f-424f-abe6-4583999f5a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12983/3214298910.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mp_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12983/2801156720.py\u001b[0m in \u001b[0;36mcompute_prob\u001b[0;34m(model, xx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# compute p(1|x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# squeeze() removes extraneous dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12983/195676802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)"
     ]
    }
   ],
   "source": [
    "p_hat = compute_prob(model, th); p_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e1b4da8-59b9-4a1c-af8c-ac4fd0dddcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_p_values(model):\n",
    "    theta = st.expon.rvs(size=len(test_data))\n",
    "    \n",
    "    p_hat = compute_prob(model, theta)#model evaluated at theta\n",
    "    p_calculated = [p_calculated(theta = theta[i]) for i in range(len(theta))]\n",
    "    \n",
    "    plt.hist(p_hat, label = r'$\\hat{p}$', alpha=0.3)\n",
    "    plt.hist(p_calculated, label='$p$ calculated', alpha=0.3)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7037e065-f805-41c5-b14f-808cd701ea4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12983/3597554309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_p_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12983/2637657473.py\u001b[0m in \u001b[0;36mcompare_p_values\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#model evaluated at theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mp_calculated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp_calculated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12983/3437482318.py\u001b[0m in \u001b[0;36mcompute_prob\u001b[0;34m(model, xx)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# compute p(1|x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# squeeze() removes extraneous dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12983/195676802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x25000 and 1x128)"
     ]
    }
   ],
   "source": [
    "compare_p_values(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feca878-2070-4522-b88c-ddbb329d37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Algorithm2(D=2, theta_0):\n",
    "    \n",
    "    \n",
    "    \n",
    "    return actual_p_value, regressed_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca910987-45ca-4197-9d17-707287948334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_hat(T):\n",
    "    \"\"\"The expectation value of Z as a relative frequency, this should equal p_hat, the learned parameterized distribution at a given theta\"\"\"\n",
    "    num = np.array(T[1]).sum()\n",
    "    den = Bprime\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01507c62-bd5c-4889-b05d-015768d8a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
