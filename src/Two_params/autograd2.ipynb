{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608d7fa3-3c37-455e-a12a-7396190f6cae",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### 1) Understanding Gradient Descent - Single and Multiple\n",
    "### 2) Understanding Batchsize\n",
    "### 3) Implementing Loss Function, Gradient Descent, Batchsize and Training from Scratch\n",
    "### 4) Autograd Within Pytorch\n",
    "### 5) Applications of SGD, Autograd and Loss Functions to our Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "051def04-6420-4753-ba8b-a4260d213189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp\n",
    "mp.rcParams['agg.path.chunksize'] = 10000\n",
    "import matplotlib.pyplot as plt\n",
    "# force inline plots\n",
    "# %matplotlib inline\n",
    "plt.style.use('seaborn-deep')\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import utils\n",
    "import joblib as jb\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 18\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\":FONTSIZE,\n",
    "})\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "# seed = 128\n",
    "# rnd  = np.random.RandomState(seed)\n",
    "    \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48459bd-619a-4953-aa80-e5f401bffaec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Undestanding SGD: one parameter\n",
    "\n",
    "Suppose we have 3 observed datapoints: 3 x points and 3 y points $\\{ \\mathbf{x}, \\mathbf{y} \\}$ and we want to fit a model that will reveal the relationship between x and y. We must start with a hypothesis for the output (y) given input $x$. The simplest hypothesis, or model is \n",
    "\n",
    "$$model(x) =f(x) = w_1 x,$$ \n",
    "where $w_1$ is a weight (just a parameter). we then need to define a loss function $L(w_1)$ which measures how good or bad the parameter $w_1$ is given observed values $x$. The simplest loss is quadratic (mean square error)  loss\n",
    "\n",
    "$$L(w_1) = \\frac{1}{2} \\sum_{i=1}^{n_{points}=3} (w_1 x_i -y_i)^2 $$\n",
    "\n",
    "The lower the loss function, the better our model at predicting the relationship between $x$ and $y$ (since lower loss function means $model(x) \\approx y$). Now the goal is to find the value of $w_1$ that leads to the minimum value of the loss function, in otherwords we want\n",
    "\n",
    "$$\\text{argmin}_{w_1} L(w_1)$$\n",
    "\n",
    "For example, you can imagine for this example plotting $L(w_1)$ vs $w_1$, which will be a parabola, and the lowest value of $L(w_1)$ will be given by solving $\\frac{\\partial L(w_1)}{\\partial w_1} = 0$. You can imagine that since it's a parabola, left of the minimum the slope will be negative and to the right of the minimum the slope is positive. The general algorithm is\n",
    "\n",
    "$$ w_1 \\leftarrow w_1 - \\alpha \\frac{\\partial L(w_1)}{\\partial w_1} $$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, which tells us how bug each step is. Therefore SGD tells us that if the slope is negative we should go to the right and if the slope is positive we should go to the left; this is implemented by the negative sign above. \n",
    "\n",
    "The derivative has to be done with the chain rule of course\n",
    "\n",
    "$$\\frac{\\partial L(w_1)}{\\partial w_1} = \\frac{1}{2} \\sum_{i=1}^{n_{points}}  [ 2 (w_1 x_i -y_i) \\times \\frac{\\partial}{\\partial w_1} (w_1 x_i -y_i)] = \\sum_{i=1}^{n_{points}} (w_1 x_i -y_i) x_i$$\n",
    "\n",
    "### Undestanding SGD: multi parameter\n",
    "\n",
    "For the general case in which we have multiple, i.e. a vector of parameters $\\mathbf{w}=\\left[\\begin{array}{c}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{m}\n",
    "\\end{array}\\right]$\n",
    "we have to take the gradient $\\nabla_w L(\\mathbf{w})=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial w_{1}} \\\\\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial w_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L(\\mathbf{w})}{\\partial w_{m}}\n",
    "\\end{array}\\right]$\n",
    "\n",
    "So that the general SGD algorithm is \n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} L(\\mathbf{w})$$\n",
    "\n",
    "If we have lots of samples, we might not be able to compute the gradient for the entire dataset, so the way to solve this is to divide the data into smaller parts called \"batches\", and we find the gradient of one batch, before moving on to the next batch, until we traverse the entire dataset. When we traverse the entire dataset, this is called one \"epoch\". We keep trainig for multiple epochs to get the best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce7924-980a-4bb4-95ff-9d6bb815deb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Autograd\n",
    "\n",
    "In order to implement SGD, we must be able to take the gradient with respect to the weights. Here we review how to do that in pytorch.\n",
    "\n",
    "Suppose we have a multivariate function $f(\\mathbf{x})$, wheren the input to the function is an $n$-dimensional vector $\\mathbf{x}=[x_1, x_2, ..., x_n]^T$, and the output is a scalar. Then\n",
    "The gradiant of function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is \n",
    "$$ \\nabla_{\\mathbf{x}} f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_{1}}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{2}}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{n}}\\right]^{\\top} $$\n",
    "\n",
    "Of course, the gradiant (of a scalar function) will be a vector, and will have the same shape as $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5ade38c-f63d-479d-b3a9-a09d8773c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7.])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(8.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92f68b-07e8-4f20-a4bc-086c538919f1",
   "metadata": {},
   "source": [
    "Suppose that $y=2 \\mathbf{x}^{\\top} \\mathbf{x}$, and say we want to differentiate it with respect to the column vector $\\mathbf{x}$. Before we can calculate $\\nabla_\\mathbf{x} y(\\mathbf{x})$ we need a place to store it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f912d-5a3e-48da-9b43-fb07802f9319",
   "metadata": {},
   "source": [
    "It is important that we don't allocate new memory every time we take a derivative with respect to a parameter, since during training we update parameters millions of times so we could run out of memory quickly. Therefore we first have to say that we want to store the gradiants of this vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3c9c7307-dfaf-495c-83a8-f56a4f27aeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7.], requires_grad=True)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1d4bdc8b-34d2-4946-b8e0-5a00026ad076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f85acf5ff50>\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
       ":func:`backward` computes gradients for ``self``.\n",
       "The attribute will then contain the gradients computed and future calls to\n",
       ":func:`backward` will accumulate (add) gradients into it.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9670d93a-c85e-4f72-8c18-c178d4587d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad#default is none as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9e135768-0488-4c8a-b71f-0fcc97278964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(280., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811141d-ff98-4dc0-9d48-bf47565620a8",
   "metadata": {},
   "source": [
    "Next, we can automatically calculate the gradient of y with respect to each component of x,$\\nabla_\\mathbf{x} y(\\mathbf{x})$, by calling the function for backpropagation and printing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "15f08d73-6e23-4d49-b82c-bd86c1c8b114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12., 16., 20., 24., 28.])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5dd6b2cd-5c14-426f-89a3-ac016f1e9d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7.], requires_grad=True)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61ba75-665e-44be-aa6a-538233428297",
   "metadata": {},
   "source": [
    "Since $y=2 \\mathbf{x}^{\\top} \\mathbf{x}$, $\\nabla_\\mathbf{x} y(\\mathbf{x})=4 \\mathbf{x}$. Let's verify that pytorch calculated the gradient correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "14be206d-44ae-43cb-8071-4e5fb730ee35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f60588f6-fbcf-4760-8d8d-e6ec9cc623a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7.], requires_grad=True)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch accumulates the gradient in default, we need to clear the previous\n",
    "# values\n",
    "x.grad.zero_()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c8c47425-4bc4-4946-a840-2c5c40dfa086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the gradient of another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dfbe42ac-4b85-4e41-b0f6-3ed90aaff6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()#another function\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946d0dc-b390-4738-aebc-7de1768c381e",
   "metadata": {},
   "source": [
    "## Detaching from the computational graph\n",
    "\n",
    "Suppose the computational steps are the following:\n",
    "* $y$ is computed as a function of $x$: y=y(x)\n",
    "* $z$ is computed as a function of both $x$ and $y$: $z=z(x,y(x))$\n",
    "* Suppose we want to calculate $\\nabla_x z(x,y)$ and treat $y$ as a constant. We can discard any information about how $y$ was computed by detaching $y$ to $y'$ which discards any information on how $y$ was computed in the computational graph:\n",
    "$$detach[y(x)]=y' \\rightarrow \\nabla_x Z(x,y)|_{y'} = \\nabla_x y' \\ x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e7861-67d3-479b-b6e4-632c9f4531b8",
   "metadata": {},
   "source": [
    "Concretely, suppose that \n",
    "1. $y(x) = x * x$, and \n",
    "2. $z(x,y)=x * y(x) = x * x* x$ \n",
    "Between steps 1. and 2, we can detach $y$ so that if we calculate the gradiant of $z$ w.r.t. $x$, $y$ will be treated as a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e2c72a65-a913-4192-8bcc-929eec592575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "16b3112f-574d-4414-b1d6-add4e3137291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49.])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prime = y.detach()\n",
    "y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4d0b8a00-b13b-4bf8-88ad-d46e2e35c70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   1.,   8.,  27.,  64., 125., 216., 343.],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y_prime * x\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d93523eb-d42c-4794-bab5-1a1a1bb14aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0200ef04-0d3d-42b8-85f8-f1a6b06d9851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a8b1b54-6ab5-4b6f-b039-49796df841e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  4.,  9., 16., 25., 36., 49.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b24e233-55c4-4f30-a208-e37b8134d41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prime == x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaea394-1596-4df7-afe5-552bfe4cbe0a",
   "metadata": {},
   "source": [
    "So we verify that $y'$ is the same as $\\nabla_x z = \\nabla_x x * y = y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbb485-bf4b-4bf0-bc08-ff6e7d0b0348",
   "metadata": {},
   "source": [
    "We can also invoke backpropagation on $y$ to get $\\nabla_x y = \\nabla_x x * x = 2 * x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d50ad62a-0584-4012-91df-0570f60b1d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dfd50-9c39-4160-bc10-4836acebd117",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fc498-c3ba-462e-9c60-6abaabd0c983",
   "metadata": {},
   "source": [
    "Good. Now let's apply this to a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1152a66a-ebd4-4db0-9809-e2e1ebb10e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    return 4 * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "63f53d3f-8057-4fc0-81d5-ab6bfdabf5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6679, requires_grad=True)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ab2bb6b7-bc22-4d13-8d07-edd1a7b174fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6714, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = f(a)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5453fa9-e9e9-4598-a72f-546df3c37925",
   "metadata": {},
   "source": [
    "Now let's calculate $\\nabla_a f=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "069b8358-ba4b-4f82-af09-f3e13d6702c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7eea874b-b87e-4b4f-b15c-6ef74c6e6dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to double check, this is the same as f/a\n",
    "f/a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c81f8-389e-4059-b61e-f4358fde45c1",
   "metadata": {},
   "source": [
    "With this knowledge in hand, we can implement SGD ourselves for any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4179b2-d56a-4730-b195-f682f84bba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"stochastic gradient descent: estimate the gradient of the loss with respect to some parameters params\n",
    "    We then update our parameters in the direction that may reduce the loss\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size#we divide by batch_size to normalize\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034b5b35-5771-4579-a8b3-5d0fa5f5497f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate Data and implement a simple Linear Regression Model\n",
    "To apply the things we learned, let's sudy the example of a linear regression model, and implement all aspects in the ML chain from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb638be8-b278-48ba-95b0-95368577dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2737e+00,  1.7328e-01,  1.1156e-01,  2.3508e-01],\n",
       "        [-1.0235e+00, -9.6387e-01,  5.9923e-01, -9.1092e-04],\n",
       "        [-3.0900e+00, -5.8915e-01, -9.3988e-01,  2.0903e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0, 1, (3, 4))#example of generating data in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010ccf8-e3bd-4a87-8170-0c6c8371ead7",
   "metadata": {},
   "source": [
    "Let's generate data for a linear regression model \n",
    "\n",
    "$$y = w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "where $b$ is the bias. In other words we want the $x$ matrix to have the shape = (nrows,ncolumns) = $ \\text{n_samples} \\times \\text{n_weights}$, so that\n",
    "$$ y = X_{\\{ N_\\text{samples} \\times N_\\text{weights} \\} } \\ \\mathbf{w}_{\\{ N_\\text{weights} \\times 1 \\}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9b003b-1ca2-4a89-81b5-c64ea1febcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features =  tensor([[-0.0059,  1.2263],\n",
      "        [-0.2093, -1.1955],\n",
      "        [ 0.8276, -0.3928],\n",
      "        [-1.8760,  0.0204],\n",
      "        [ 0.8637, -0.2863]]) \n",
      "\n",
      " labels=  tensor([[0.0237],\n",
      "        [7.8296],\n",
      "        [7.1975],\n",
      "        [0.3750],\n",
      "        [6.9007]])\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)# noise term\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "print('features = ', features[:5], '\\n\\n', 'labels= ', labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164edef-9e31-4ffa-9454-5e68025f8c9e",
   "metadata": {},
   "source": [
    "## Minibatch\n",
    "The concept of minibatches is fundamental to training any ML model, as discussed the SGD section above. Let's generate data and access them in batches. schematically, the **data_iter** function does the following mapping:\n",
    "\n",
    "**data_iter :** {features, labels, batch_size}  $\\rightarrow$ features[batch_index], labels[batch_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc63ed9-a5e9-4fa6-bc33-3428ba797dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde29264-6bc7-48de-a0f5-224a6998b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features in one batch =  tensor([[ 0.3043, -0.2010],\n",
      "        [ 1.6679, -0.2868],\n",
      "        [ 1.7942, -0.7574],\n",
      "        [-0.9829,  2.4098],\n",
      "        [-1.3588, -0.8608],\n",
      "        [-1.1160, -0.3118],\n",
      "        [ 0.0735, -0.1023],\n",
      "        [ 1.8569,  0.7529],\n",
      "        [-0.4662,  0.0900],\n",
      "        [ 1.2087,  0.3810]]) \n",
      " labels in one batch=  tensor([[ 5.4823],\n",
      "        [ 8.4956],\n",
      "        [10.3777],\n",
      "        [-5.9581],\n",
      "        [ 4.4028],\n",
      "        [ 3.0308],\n",
      "        [ 4.6812],\n",
      "        [ 5.3460],\n",
      "        [ 2.9653],\n",
      "        [ 5.3138]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print('features in one batch = ', X, '\\n', 'labels in one batch= ',y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910f544-1382-4afc-ad93-b62c5dd5e282",
   "metadata": {},
   "source": [
    "make linear regression model as discussed above\n",
    "$$y = w_1 x_1 + w_2 x_2 + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82bb021-5984-47ea-9426-8e59251b4c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(2,1))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb9ae791-7e96-45a9-8ba6-4bc7da6225fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6858, -0.2013]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight#0 is the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07a0c4a5-8112-4d13-ad41-e57449216c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6469], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5139eb-6e08-47ba-bf51-9dab812231fa",
   "metadata": {},
   "source": [
    "### Define Loss Function\n",
    "Let's use quardatic loss,   function was $L(\\mathbf{w}|x) =  (t - model(x))^2$ Where $t$ is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "653e78de-23df-4389-a53b-f3104ea77971",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################PYTORCH###############\n",
    "loss_pytorch = nn.MSELoss()\n",
    "\n",
    "\n",
    "#################FROM SCRATCH##############\n",
    "def loss_fromscratch(f, t):\n",
    "    # f and t must be of the same shape\n",
    "    return  torch.mean((f - t)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984effe-d287-498a-9af4-50d16dc4e884",
   "metadata": {},
   "source": [
    "### Define \"optimizer\", or \"trainer\"\n",
    "\n",
    "This is the thing that implements our optimization algorithm, like SGD. There are so many algorithms for optimization, and Adam has proven to be the most effective for the widest range of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "768d7d0e-f7b0-4e01-a7e1-744ddfff553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.e-3\n",
    "\n",
    "optimizer_pytorch = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "# optimizer     = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63be9c-1f04-4043-85e3-b90dd931219e",
   "metadata": {},
   "source": [
    "Train: train over entire dataset for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "525086ac-7c40-45dd-b59f-fe4bd8ddab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 16.948315\n",
      "epoch 2, loss 11.118738\n",
      "epoch 3, loss 7.297488\n"
     ]
    }
   ],
   "source": [
    "n_epochs=3\n",
    "loss = loss_pytorch\n",
    "optimizer = optimizer_pytorch\n",
    "for epoch in range(n_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X), y)#compare prediction to targets\n",
    "        optimizer.zero_grad()#empty out the gradients\n",
    "        l.backward()#calculate the gradient of the loss function \n",
    "        optimizer.step()#update the values of the parameters (weights) of model\n",
    "        \n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch+1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e0782-20e6-4879-bd32-d4643f0b1b10",
   "metadata": {},
   "source": [
    "-------\n",
    "Now, back to out project. Recall that the target, $Z$, is either $0$ or $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07b60f7e-86b0-48da-a58d-652c3c67f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bprime    = 10000\n",
    "thetaMin, thetaMax =  0, 10\n",
    "nuMin, nuMax = 0, 10\n",
    "Mmin, Mmax =  0 , 10\n",
    "Nmin, Nmax =  0,10\n",
    "MLE=True\n",
    "def generate_Z_lambda(Bprime, save_data=False):\n",
    "    \"\"\"Generate the training data, that is, features=[theta, nu, N, M], targets=Z\"\"\"\n",
    "    #sample theta and nu from uniform(0,20)\n",
    "    theta = st.uniform.rvs(thetaMin, thetaMax, size=Bprime)\n",
    "    nu = st.uniform.rvs(nuMin, nuMax, size=Bprime)\n",
    "    #n,m ~ F_{\\theta,\\nu}, ie our simulator. sample n from a Poisson with mean theta+nu \n",
    "    n = st.poisson.rvs(theta+ nu, size=Bprime)\n",
    "    #sample m from a poisson with mean nu\n",
    "    m = st.poisson.rvs(nu, size=Bprime)\n",
    "    \n",
    "    #sample our observed counts (N,M), which take the place of D\n",
    "    N = np.random.randint(Nmin, Nmax, size=Bprime)\n",
    "    M = np.random.randint(Mmin, Mmax, size=Bprime)\n",
    "    # print('n=', n)\n",
    "    # print('m=', m)\n",
    "    # print('N=', N)\n",
    "    # print('M=', M)\n",
    "    lambda_gen = utils.lambda_test(theta, n, m, MLE)\n",
    "    # print('lambda_gen= ', lambda_gen)\n",
    "    lambda_D = utils.lambda_test(theta, N, M, MLE)\n",
    "    # print('lambda_D= ', lambda_D)\n",
    "    #if lambda_gen <= lambda_D: Z=1, else Z=0\n",
    "    Z = (lambda_gen <= lambda_D).astype(np.int32)\n",
    "    \n",
    "\n",
    "    return lambda_gen, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "531176c2-bf67-4462-af5e-0d764cbca563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$Z$')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambda_gen, Z = generate_Z_lambda(1000)\n",
    "\n",
    "plt.scatter(lambda_gen, Z)\n",
    "plt.xlabel(r'$\\lambda (\\theta, n, m, \\hat{\\nu}(\\theta))$'); plt.ylabel(r'$Z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45b553-71d1-4c80-82ae-90b195b62b86",
   "metadata": {},
   "source": [
    "And our loss function was $L(t, model) =  (t - model(\\theta, \\nu, n, m))^2$ Where $t$ is the label of $Z$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7fef3-2147-421b-a479-a446b4bbc20c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, with inputs $\\{ \\theta, \\nu, N, M \\}$, if we vary $\\nu$ in the histogram as well as in the model we observe that:\n",
    "1. In the case of MLE, there is insensetivity to $\\nu$.\n",
    "2. for the non-MLE, the probability does depend on $\\nu$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14334b-8ce4-4ed5-a82b-882c94cb8cd3",
   "metadata": {},
   "source": [
    "However recall that the output of our model is the approxmation $E(Z|\\theta, \\nu, N, M)$ where $\\{ \\theta, \\nu, N, M \\}$ are the input features. Since $\\nu$ is a nuissance parameter, we would like a function $\\lambda(\\theta,\\nu, N,M)$ such that the expectation $E(Z|\\theta, \\nu, N, M)$ is independent of nuissance parameter $\\nu$. One example is that we can impose this condition by\n",
    "\n",
    "$$\\frac{\\partial E(Z|\\theta,\\nu,N,M)}{\\partial \\nu} =0                               \\tag{1}$$\n",
    "for all $\\theta, N,M$. \n",
    "\n",
    "The question then becomes:\n",
    "\n",
    "Can we replace $\\lambda$ by a neural network with the condition above, but this time the neural network has an extra input $\\hat{\\theta}$. That is, the NN woud have $\\{ \\theta, \\nu, N, M, \\hat{\\theta} \\}$ as input. In doing this, wWee hope that:\n",
    "1. If we put $\\hat{\\theta}$ as the MLE ($\\hat{\\theta}_{MLE}$), the NN for $\\lambda$ would reproduce the analytical $\\lambda$. \n",
    "2. We also hope that if we were to put non-MLE $\\hat{\\theta}_{non-MLE}$ the NN would have minimal sensitivity to $\\nu$. \n",
    "\n",
    "This can be done, for example by adding an additional term to the loss function which is proportional to the square of the condition above\n",
    "\n",
    "$$L(t, model) =  (t - model(\\theta, \\hat{\\theta}, \\nu, n, m))^2 + \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu} \\right\\|^2 \\tag{2}$$\n",
    "\n",
    "Or\n",
    "$$L(t, model) =  \\left( sigmoid(t) - model(\\theta, \\hat{\\theta}, \\nu, n, m) \\right)^2 + \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu} \\right\\|^2 \\tag{2}$$\n",
    "\n",
    "\n",
    "We can impose the condition (1) by the additive term in the loss function (2), where $\\kappa$ is a regularization constant, a non-negative hyperparameter that we can fit using validation data. For $\\kappa=0$ we recover our original loss function. For $\\kappa>0$, we restrict the size of the $L_2$ norm of $\\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu}$, i.e. we restrict $\\| \\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu} \\|^2$. The factor of $1/2$ is just by convention, since after we take the derivative of the loss function, the $2$ and the $1/2$ cancel out. Also, we use the square norm for computational convenience, it is much easier to compute for vectors than if we keep the square root in there!\n",
    "\n",
    "(Recall that the $L_2$ norm for a vector $\\mathbf{x}$ is like a Euclidean distance, $L_2=\\|\\mathbf{x}\\|_{2}=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}$, which is a case f the general $L_p$ norm $\\|\\mathbf{x}\\|_{p}=\\left(\\sum_{i=1}^{n}\\left|x_{i}\\right|^{p}\\right)^{1 / p}$. Further, norms are functions that tell us how big a vector is, so by adding $\\left\\| \\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu} \\right|^2$ we are restricting the size of $\\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu}$ .)\n",
    "\n",
    "The intuition for this is that if we have those two terms in the loss function, and we minimize w.r.t those two terms, that we are forcing the gradient descent into parameter space that has minimal sensetivity to $\\nu$. Such that if there exists a statistic $\\hat{\\hat{\\lambda}}$ which is perfectly independent of $\\nu$, even for the non-MLE case, we expect that derivative to be 0 everywhere. \n",
    "\n",
    "-------------\n",
    "### Attempt at implementation\n",
    "\n",
    "Proposed solution:\n",
    "*  initialize values for $\\{ \\theta,\\nu,n,m \\}$, requires_grad=True.\n",
    "* Calculate a simple model (net) at those features, e.g. $model(\\{ \\theta,\\nu,n,m \\})$, requires_grad=True\n",
    "* Detach the calculation of the weights\n",
    "* calculate the gradient of model wrt nu, and use that in the loss function\n",
    "$L(t, model) =  (t - model(\\theta, \\hat{\\theta}, \\nu, n, m))^2 + \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial model(\\theta,\\nu,\\hat{\\theta},N,M)}{\\partial \\nu} \\right\\|^2$.\n",
    "* Now allow the calculation of the weights again (attach) and continue with SQG and other parts of the training, keeping in mind that the *step* which is invoked in the training is only with respect to the model parameters (weights) and not $\\nu$.\n",
    "\n",
    "\n",
    "### Let's consider a very simple solution: using a linear regression example\n",
    "\n",
    "$$\\hat{\\mathbf{y}}=\\mathbf{X} \\mathbf{w}+b$$\n",
    "\n",
    "Or, we could try $$\\hat{\\mathbf{y}}=sigmoid(\\mathbf{X} \\mathbf{w}+b)$$ since $\\hat{y}$ is approximating $Z$ which is an indicator function.\n",
    "Most popular loss function in regression is the squared loss \n",
    "$$l^{(i)}(\\mathbf{w}, b)=\\frac{1}{2}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^{2} + \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\nu^{(i)}} \\right\\|^2$$\n",
    "The half is just for convenience to cancel the 2 when we take derivative of the loss function.\n",
    "The empirical risk, or objective function, over the entire dataset of $n$ examples is\n",
    "$$R(\\mathbf{w}, b)=\\frac{1}{n} \\sum_{i=1}^{n} l^{(i)}(\\mathbf{w}, b)$$\n",
    "$$R(\\mathbf{w},b)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b+ \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\nu^{(i)}} \\right\\|^2-y^{(i)}\\right)^{2}$$\n",
    "\n",
    "We wish to minimize the training model to find the parameters $\\mathbf{w}^*, b^*$ that minimize the total loss across all training examples\n",
    "$$\\mathbf{w}^{*}, b^{*}=\\underset{\\mathbf{w}, b}{\\operatorname{argmin}} R(\\mathbf{w}, b) .$$\n",
    "\n",
    "\n",
    "Minibatch gradient descient: first randomly sample a monibatch $B$ consisting of a fixed number of training examples. We first randomly initialize $\\mathbf{w}, b$ and then do the following updating\n",
    "$$\n",
    "\\mathbf{w}  \\leftarrow \\mathbf{w}-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b)=\\mathbf{w}-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b+ \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\nu^{(i)}} \\right\\|^2-y^{(i)}\\right), $$\n",
    "\n",
    "$$b \\leftarrow b-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{b} l^{(i)}(\\mathbf{w}, b)=b-\\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b+ \\frac{\\kappa}{2} \\ \\left\\| \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\nu^{(i)}} \\right\\|^2-y^{(i)}\\right)$$\n",
    "\n",
    "$|\\mathcal{B}|$ represents the number of examples in the minibatch (batchsize), and $\\eta$ is the learning rate.\n",
    "\n",
    "\n",
    "In ML, in order to satisfty constrained optimization, there are several methods that have to do with convexity or penalties. \n",
    "\n",
    "Suppose you want to ensure that the weights $\\mathbf{w}$ do not grow too large, then you add a term like $\\frac{\\lambda}{2} \\|\\mathbf{w} \\|^2 $. This ensures that $\\| \\mathbf{w} \\|^2 -r^2 \\le 0$ for some radius $r$ and adjusting $\\lambda$ allows us to vary the size of $\\mathbf{w}$.\n",
    "\n",
    "Another possibility is through the **forward propagation**. Forward propagation refers to the calculation and storage of intermediate variables from input to output layers. \n",
    "\n",
    "Here, it might be helpful to plot the computational graph to help us visualize the dependencies of operators and variables in the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9516872-97b6-4d1f-bddd-241f212f4d39",
   "metadata": {},
   "source": [
    "### Activation Functions and Their Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6df21202-c08e-4be3-8227-722bdd108c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 3, sharey=True, figsize=(13,7))\n",
    "axes = axes.flatten()\n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "########RELU\n",
    "relu = torch.relu(x)\n",
    "axes[0].plot(x.detach(), relu.detach(),label=r'$ReLU(x) \\equiv max(x,0)$')\n",
    "#take derivative\n",
    "relu.backward(torch.ones_like(x), retain_graph=True)\n",
    "axes[0].plot(x.detach(), x.grad, label=r'$\\frac{\\partial \\ ReLU(x)}{\\partial x}$')\n",
    "######SIGMOID\n",
    "sigmoid = torch.sigmoid(x)\n",
    "axes[1].plot(x.detach(), sigmoid.detach(),label=r'Sigmoid $(x)$')\n",
    "# Clear out previous gradients\n",
    "x.grad.data.zero_()\n",
    "#take derivative\n",
    "sigmoid.backward(torch.ones_like(x), retain_graph=True)\n",
    "axes[1].plot(x.detach(), x.grad, label=r'$\\frac{\\partial \\ Sigmoid(x)}{\\partial x}$')\n",
    "###tanh\n",
    "tanh = torch.tanh(x)\n",
    "axes[2].plot(x.detach(), tanh.detach(), label=r'tanh $(x)$')\n",
    "x.grad.data.zero_()\n",
    "tanh.backward(torch.ones_like(x),retain_graph=True)\n",
    "axes[2].plot(x.detach(), x.grad, label=r'$\\frac{\\partial \\ tanh(x)}{\\partial x}$')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylim(-0.2,2.5)\n",
    "    ax.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dcbcd-fd9d-40b0-bc2b-a2801a28b645",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ffe4400-bafe-4174-a67b-f252e1f95d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>N</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.755366</td>\n",
       "      <td>18.032972</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.864009</td>\n",
       "      <td>15.171221</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.642570</td>\n",
       "      <td>9.228130</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.952433</td>\n",
       "      <td>0.052783</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.697489</td>\n",
       "      <td>18.611853</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509995</th>\n",
       "      <td>8.378316</td>\n",
       "      <td>3.066973</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509996</th>\n",
       "      <td>12.520709</td>\n",
       "      <td>2.888144</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509997</th>\n",
       "      <td>2.273833</td>\n",
       "      <td>1.704000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509998</th>\n",
       "      <td>17.995255</td>\n",
       "      <td>6.827394</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509999</th>\n",
       "      <td>10.006750</td>\n",
       "      <td>5.012523</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            theta         nu  N  M\n",
       "0       11.755366  18.032972  4  2\n",
       "1       10.864009  15.171221  5  7\n",
       "2        1.642570   9.228130  1  1\n",
       "3        2.952433   0.052783  0  9\n",
       "4        4.697489  18.611853  9  0\n",
       "...           ...        ... .. ..\n",
       "509995   8.378316   3.066973  3  4\n",
       "509996  12.520709   2.888144  2  5\n",
       "509997   2.273833   1.704000  3  2\n",
       "509998  17.995255   6.827394  4  3\n",
       "509999  10.006750   5.012523  2  4\n",
       "\n",
       "[510000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLE_TRUE_FILE = '../../tutorials/data1.db'\n",
    "MLE_FALSE_FILE = '../../tutorials/data2.db'\n",
    "#For both, Z for the two parameter problem is Z2\n",
    "MLE=True\n",
    "\n",
    "if MLE:\n",
    "    datafile = MLE_TRUE_FILE\n",
    "else:\n",
    "    datafile = MLE_FALSE_FILE\n",
    "    \n",
    "data = jb.load(MLE_FALSE_FILE)\n",
    "features = data[['theta', 'nu', 'N', 'M']]\n",
    "target = data['Z2']\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf169b-8c11-4025-9103-d72882272ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
